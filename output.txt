Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/05/22 16:33:59 WARN Utils: Your hostname, dvaughn-linux resolves to a loopback address: 127.0.1.1; using 172.16.2.30 instead (on interface eno1)
17/05/22 16:33:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
17/05/22 16:34:05 INFO SparkContext: Running Spark version 2.1.0
17/05/22 16:34:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/22 16:34:05 INFO SecurityManager: Changing view acls to: root
17/05/22 16:34:05 INFO SecurityManager: Changing modify acls to: root
17/05/22 16:34:05 INFO SecurityManager: Changing view acls groups to: 
17/05/22 16:34:05 INFO SecurityManager: Changing modify acls groups to: 
17/05/22 16:34:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/05/22 16:34:05 INFO Utils: Successfully started service 'sparkDriver' on port 36463.
17/05/22 16:34:05 INFO SparkEnv: Registering MapOutputTracker
17/05/22 16:34:05 INFO SparkEnv: Registering BlockManagerMaster
17/05/22 16:34:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/22 16:34:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/22 16:34:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a9161e8c-e05e-44dc-b58a-7356147970d9
17/05/22 16:34:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/05/22 16:34:05 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/22 16:34:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/22 16:34:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.2.30:4040
17/05/22 16:34:06 INFO SparkContext: Added file file:/home/david/code/python/DeepATS/./elephas/run.py at file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495485246063
17/05/22 16:34:06 INFO Utils: Copying /home/david/code/python/DeepATS/elephas/run.py to /tmp/spark-9fd13e42-38ac-4439-ab73-707e070fd470/userFiles-b6fea3ed-db7d-4828-935c-0dac8f17b0a0/run.py
17/05/22 16:34:06 INFO Executor: Starting executor ID driver on host localhost
17/05/22 16:34:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35363.
17/05/22 16:34:06 INFO NettyBlockTransferService: Server created on 172.16.2.30:35363
17/05/22 16:34:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/22 16:34:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.30, 35363, None)
17/05/22 16:34:06 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.30:35363 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.30, 35363, None)
17/05/22 16:34:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.30, 35363, None)
17/05/22 16:34:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.30, 35363, None)
>>> Hyperas search space:

def get_space():
    return {
        'lr': hp.lognormal('lr', -3 * 2.3, .8),
        'rho': hp.normal('rho', .875, .04),
        'clipnorm': hp.uniform('clipnorm', 1, 15),
        'emb_dim': hp.choice('emb_dim', [50, 100, 200, 300]),
        'rnn_dim': hp.uniform('rnn_dim', 50, 300),
    }

>>> Data
   1: 
   2: from keras.utils import np_utils
   3: from keras.preprocessing import sequence
   4: import keras.backend as K
   5: import numpy as np
   6: 
   7: import pickle as pk
   8: import deepats.asap_reader as dataset
   9: from deepats.w2vEmbReader import W2VEmbReader as EmbReader
  10: from deepats.config import get_args
  11: 
  12: import logging
  13: logger = logging.getLogger(__name__)
  14: 
  15: args = get_args()
  16: 
  17: if args.seed > 0:
  18:     np.random.seed(args.seed)
  19:     
  20: emb_reader = EmbReader(args.emb_path, emb_dim=args.emb_dim)
  21: emb_words = emb_reader.load_words()
  22: 
  23: dataset.set_score_range(args.data_set)
  24: (train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data((args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path, min_word_freq=args.min_word_freq, emb_words=emb_words)
  25: 
  26: abs_vocab_file = args.abs_out_path + 'vocab.pkl'
  27: with open(abs_vocab_file, 'wb') as vocab_file:
  28:     pk.dump(vocab, vocab_file)
  29: 
  30: train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)
  31: dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)
  32: test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)
  33: 
  34: train_y = np.array(train_y, dtype=K.floatx())
  35: dev_y = np.array(dev_y, dtype=K.floatx())
  36: test_y = np.array(test_y, dtype=K.floatx())
  37: 
  38: if args.prompt_id:
  39:     train_pmt = np.array(train_pmt, dtype='int32')
  40:     dev_pmt = np.array(dev_pmt, dtype='int32')
  41:     test_pmt = np.array(test_pmt, dtype='int32')
  42: 
  43: dev_y_org = dev_y.astype(dataset.get_ref_dtype())
  44: test_y_org = test_y.astype(dataset.get_ref_dtype())
  45: 
  46: train_y = dataset.get_model_friendly_scores(train_y, train_pmt)
  47: dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)
  48: test_y = dataset.get_model_friendly_scores(test_y, test_pmt)
  49: 
  50: 
  51: 
  52: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     
   4:     
   5:     def random_id(size=6, chars=string.ascii_uppercase + string.digits):
   6:         return ''.join(random.choice(chars) for _ in range(size))
   7:     
   8:     ms = int(round(time.time() * 1000))
   9:     rand_seed = ms % (2**32 - 1)
  10:     random.seed(rand_seed)
  11:        
  12:     args = get_args()
  13:     model_id = random_id()
  14:     
  15:     def kappa_metric(t,x):
  16:         u = 0.5 * K.sum(K.square(x - t))
  17:         v = K.dot(K.transpose(x), t - K.mean(t))
  18:         return v / (v + u)
  19:     
  20:     def kappa_loss(t,x):
  21:         u = K.sum(K.square(x - t))
  22:         v = K.dot(K.squeeze(x,1), K.squeeze(t - K.mean(t),1))
  23:         return u / (2*v + u)
  24:     
  25:     lr = space['lr']
  26:     lr = lr*2
  27:     rho = space['rho']
  28:     clipnorm=space['clipnorm']
  29:     eps=1e-6
  30:     
  31:     opt = optimizers.RMSprop(lr=lr,
  32:                             rho=rho,
  33:                             clipnorm=clipnorm,
  34:                             epsilon=eps
  35:                             )
  36:     loss = kappa_loss
  37:     metric = kappa_metric
  38:     dataset.set_score_range(args.data_set)
  39:     evl = Evaluator(dataset, args.prompt_id, args.abs_out_path, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org, model_id=model_id)
  40:     
  41:     abs_vocab_file = args.abs_out_path + 'vocab.pkl'
  42:     with open(abs_vocab_file, 'rb') as vocab_file:
  43:         vocab = pk.load(vocab_file)
  44:     
  45:     train_y_mean = train_y.mean(axis=0)
  46:     if train_y_mean.ndim == 0:
  47:         train_y_mean = np.expand_dims(train_y_mean, axis=1)
  48:     num_outputs = len(train_y_mean)
  49:     
  50:     mask_zero=False
  51:     
  52:     emb_dim = space['emb_dim']
  53:     rnn_dim = space['rnn_dim']
  54:     rnn_dim = int(rnn_dim)
  55:                    
  56:     model = Sequential()
  57:     model.add(Embedding(args.vocab_size, emb_dim, mask_zero=mask_zero))
  58:     model.add(RWA(rnn_dim))
  59:     model.add(Dense(num_outputs))
  60:     if not args.skip_init_bias:
  61:         bias_value = (np.log(train_y_mean) - np.log(1 - train_y_mean)).astype(K.floatx())
  62:         model.layers[-1].bias.set_value(bias_value)
  63:     model.add(Activation('tanh'))
  64:     model.emb_index = 0
  65:     
  66:     emb_path = 'embed/glove.6B.{}d.txt'.format(emb_dim)
  67:     abs_emb_path = args.abs_root + emb_path
  68:     
  69:     emb_reader = EmbReader(abs_emb_path, emb_dim=emb_dim)
  70:     emb_reader.load_embeddings(vocab)
  71:     emb_wts = emb_reader.get_emb_matrix_given_vocab(vocab, model.layers[model.emb_index].get_weights()[0])
  72:     wts = model.layers[model.emb_index].get_weights()
  73:     wts[0] = emb_wts
  74:     model.layers[model.emb_index].set_weights(wts)
  75: 
  76:     model.compile(loss=loss, optimizer=opt, metrics=[metric])
  77:     model_yaml = model.to_yaml()
  78:     print('model_id: %s' % (model_id))
  79:     print(model_yaml)
  80:     print('optimizer:    lr= %.4f, rho= %.4f, clipnorm= %.4f, epsilon= %.4f' % (lr, rho, clipnorm, eps))
  81:     
  82:     print('PARAMS\t\
  83:     %s\t\
  84:     lr= %.4f\t\
  85:     rho= %.4f\t\
  86:     clip= %.4f\t\
  87:     emb= %.4f\t\
  88:     rnn= %.4f' % (model_id, lr, rho, clipnorm, emb_dim, rnn_dim))
  89:     
  90:     for i in range(args.epochs):
  91:         train_history = model.fit(train_x, train_y, batch_size=args.batch_size, epochs=1, verbose=0)
  92:         evl.evaluate(model, i)
  93:         evl.output_info()
  94:         if i>5 and evl.dev_metric<0.4:
  95:             break
  96:         if i>10 and evl.dev_metric<0.5:
  97:             break
  98:         if i>15 and evl.dev_metric<0.6:
  99:             break
 100: 
 101:     best_dev_kappa = evl.best_dev
 102:     best_test_kappa = evl.best_test
 103: 
 104:     print('Test kappa:', best_dev_kappa)
 105:     return {'loss': 1-best_dev_kappa, 'status': STATUS_OK, 'model': model.to_yaml(), 'weights': pk.dumps(model.get_weights())}
 106: 
17/05/22 16:34:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:34:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KB, free 366.3 MB)
17/05/22 16:34:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.2.30:35363 (size: 3.8 KB, free: 366.3 MB)
17/05/22 16:34:06 INFO SparkContext: Created broadcast 0 from broadcast at PythonRDD.scala:482
17/05/22 16:34:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:34:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 114.0 B, free 366.3 MB)
17/05/22 16:34:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.2.30:35363 (size: 114.0 B, free: 366.3 MB)
17/05/22 16:34:06 INFO SparkContext: Created broadcast 1 from broadcast at PythonRDD.scala:482
17/05/22 16:34:06 INFO SparkContext: Starting job: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27
17/05/22 16:34:06 INFO DAGScheduler: Registering RDD 2 (coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:34:06 INFO DAGScheduler: Got job 0 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) with 2 output partitions
17/05/22 16:34:06 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:34:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/05/22 16:34:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/05/22 16:34:06 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents
17/05/22 16:34:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 366.3 MB)
17/05/22 16:34:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KB, free 366.3 MB)
17/05/22 16:34:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.2.30:35363 (size: 3.2 KB, free: 366.3 MB)
17/05/22 16:34:06 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/05/22 16:34:06 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:34:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/05/22 16:34:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:34:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:34:06 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 6462 bytes)
17/05/22 16:34:06 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:34:06 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:34:06 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:34:06 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:34:06 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 6508 bytes)
17/05/22 16:34:06 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/05/22 16:34:06 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/05/22 16:34:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/22 16:34:06 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/05/22 16:34:06 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/05/22 16:34:06 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/05/22 16:34:06 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/05/22 16:34:06 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/05/22 16:34:06 INFO Executor: Fetching file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495485246063
17/05/22 16:34:06 INFO Utils: /home/david/code/python/DeepATS/./elephas/run.py has been previously copied to /tmp/spark-9fd13e42-38ac-4439-ab73-707e070fd470/userFiles-b6fea3ed-db7d-4828-935c-0dac8f17b0a0/run.py
17/05/22 16:34:07 INFO PythonRunner: Times: total = 206, boot = 181, init = 25, finish = 0
17/05/22 16:34:07 INFO PythonRunner: Times: total = 177, boot = 170, init = 6, finish = 1
17/05/22 16:34:07 INFO PythonRunner: Times: total = 193, boot = 179, init = 14, finish = 0
17/05/22 16:34:07 INFO PythonRunner: Times: total = 194, boot = 188, init = 6, finish = 0
17/05/22 16:34:07 INFO PythonRunner: Times: total = 176, boot = 175, init = 1, finish = 0
17/05/22 16:34:07 INFO PythonRunner: Times: total = 194, boot = 177, init = 17, finish = 0
17/05/22 16:34:07 INFO PythonRunner: Times: total = 175, boot = 172, init = 2, finish = 1
17/05/22 16:34:07 INFO PythonRunner: Times: total = 176, boot = 169, init = 6, finish = 1
17/05/22 16:34:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1969 bytes result sent to driver
17/05/22 16:34:07 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1969 bytes result sent to driver
17/05/22 16:34:07 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1969 bytes result sent to driver
17/05/22 16:34:07 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1882 bytes result sent to driver
17/05/22 16:34:07 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1969 bytes result sent to driver
17/05/22 16:34:07 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1969 bytes result sent to driver
17/05/22 16:34:07 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1969 bytes result sent to driver
17/05/22 16:34:07 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1969 bytes result sent to driver
17/05/22 16:34:07 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 417 ms on localhost (executor driver) (1/8)
17/05/22 16:34:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 451 ms on localhost (executor driver) (2/8)
17/05/22 16:34:07 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 422 ms on localhost (executor driver) (3/8)
17/05/22 16:34:07 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 430 ms on localhost (executor driver) (4/8)
17/05/22 16:34:07 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 421 ms on localhost (executor driver) (5/8)
17/05/22 16:34:07 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 426 ms on localhost (executor driver) (6/8)
17/05/22 16:34:07 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 419 ms on localhost (executor driver) (7/8)
17/05/22 16:34:07 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 431 ms on localhost (executor driver) (8/8)
17/05/22 16:34:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/05/22 16:34:07 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.469 s
17/05/22 16:34:07 INFO DAGScheduler: looking for newly runnable stages
17/05/22 16:34:07 INFO DAGScheduler: running: Set()
17/05/22 16:34:07 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/05/22 16:34:07 INFO DAGScheduler: failed: Set()
17/05/22 16:34:07 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27), which has no missing parents
17/05/22 16:34:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.0 KB, free 366.3 MB)
17/05/22 16:34:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KB, free 366.3 MB)
17/05/22 16:34:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.2.30:35363 (size: 7.7 KB, free: 366.3 MB)
17/05/22 16:34:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/05/22 16:34:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:34:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/05/22 16:34:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 6176 bytes)
17/05/22 16:34:07 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 6176 bytes)
17/05/22 16:34:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
17/05/22 16:34:07 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
17/05/22 16:34:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:34:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
17/05/22 16:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
Using Theano backend.
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
17/05/22 16:34:18 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 8)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/05/22 16:34:19 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

17/05/22 16:34:19 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
17/05/22 16:34:19 INFO TaskSchedulerImpl: Cancelling stage 1
17/05/22 16:34:19 INFO Executor: Executor is trying to kill task 1.0 in stage 1.0 (TID 9)
17/05/22 16:34:19 INFO TaskSchedulerImpl: Stage 1 was cancelled
17/05/22 16:34:19 INFO DAGScheduler: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) failed in 11.729 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
17/05/22 16:34:19 INFO DAGScheduler: Job 0 failed: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27, took 12.295217 s
Traceback (most recent call last):
  File "/home/david/code/python/DeepATS/./elephas/run.py", line 190, in <module>
    best_model = hyperparam_model.minimize(model=model, data=data, max_evals=100)
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 32, in minimize
    trials_list = self.compute_trials(model, data, max_evals)
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 27, in compute_trials
    trials_list = dummy_rdd.mapPartitions(hyperas_worker.minimize).collect()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in collect
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:934)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

17/05/22 16:34:19 WARN PythonRunner: Incomplete task interrupted: Attempting to kill Python Worker
17/05/22 16:34:19 INFO Executor: Executor killed task 1.0 in stage 1.0 (TID 9)
17/05/22 16:34:19 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 9, localhost, executor driver): TaskKilled (killed intentionally)
17/05/22 16:34:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/05/22 16:34:19 INFO SparkContext: Invoking stop() from shutdown hook
17/05/22 16:34:19 INFO SparkUI: Stopped Spark web UI at http://172.16.2.30:4040
17/05/22 16:34:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/05/22 16:34:19 INFO MemoryStore: MemoryStore cleared
17/05/22 16:34:19 INFO BlockManager: BlockManager stopped
17/05/22 16:34:19 INFO BlockManagerMaster: BlockManagerMaster stopped
17/05/22 16:34:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/05/22 16:34:19 INFO SparkContext: Successfully stopped SparkContext
17/05/22 16:34:19 INFO ShutdownHookManager: Shutdown hook called
17/05/22 16:34:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-9fd13e42-38ac-4439-ab73-707e070fd470
17/05/22 16:34:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-9fd13e42-38ac-4439-ab73-707e070fd470/pyspark-c164e820-6b70-4666-a70b-2120078a4f42
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/05/22 16:35:11 WARN Utils: Your hostname, dvaughn-linux resolves to a loopback address: 127.0.1.1; using 172.16.2.30 instead (on interface eno1)
17/05/22 16:35:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
17/05/22 16:35:16 INFO SparkContext: Running Spark version 2.1.0
17/05/22 16:35:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/22 16:35:16 INFO SecurityManager: Changing view acls to: root
17/05/22 16:35:16 INFO SecurityManager: Changing modify acls to: root
17/05/22 16:35:16 INFO SecurityManager: Changing view acls groups to: 
17/05/22 16:35:16 INFO SecurityManager: Changing modify acls groups to: 
17/05/22 16:35:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/05/22 16:35:16 INFO Utils: Successfully started service 'sparkDriver' on port 42573.
17/05/22 16:35:16 INFO SparkEnv: Registering MapOutputTracker
17/05/22 16:35:17 INFO SparkEnv: Registering BlockManagerMaster
17/05/22 16:35:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/22 16:35:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/22 16:35:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-635cc8d7-8469-4962-8e69-b30818190af6
17/05/22 16:35:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/05/22 16:35:17 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/22 16:35:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/22 16:35:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.2.30:4040
17/05/22 16:35:17 INFO SparkContext: Added file file:/home/david/code/python/DeepATS/./elephas/run.py at file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495485317398
17/05/22 16:35:17 INFO Utils: Copying /home/david/code/python/DeepATS/elephas/run.py to /tmp/spark-3752d22a-6244-4435-88a3-02348d4672de/userFiles-65cfb66d-8829-4960-bb97-180d78baa697/run.py
17/05/22 16:35:17 INFO Executor: Starting executor ID driver on host localhost
17/05/22 16:35:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36337.
17/05/22 16:35:17 INFO NettyBlockTransferService: Server created on 172.16.2.30:36337
17/05/22 16:35:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/22 16:35:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.30, 36337, None)
17/05/22 16:35:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.30:36337 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.30, 36337, None)
17/05/22 16:35:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.30, 36337, None)
17/05/22 16:35:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.30, 36337, None)
>>> Hyperas search space:

def get_space():
    return {
        'lr': hp.lognormal('lr', -3 * 2.3, .8),
        'rho': hp.normal('rho', .875, .04),
        'clipnorm': hp.uniform('clipnorm', 1, 15),
        'emb_dim': hp.choice('emb_dim', [50, 100, 200, 300]),
        'rnn_dim': hp.uniform('rnn_dim', 50, 300),
    }

>>> Data
   1: 
   2: from keras.utils import np_utils
   3: from keras.preprocessing import sequence
   4: import keras.backend as K
   5: import numpy as np
   6: 
   7: import pickle as pk
   8: import deepats.asap_reader as dataset
   9: from deepats.w2vEmbReader import W2VEmbReader as EmbReader
  10: from deepats.config import get_args
  11: 
  12: import logging
  13: logger = logging.getLogger(__name__)
  14: 
  15: args = get_args()
  16: 
  17: if args.seed > 0:
  18:     np.random.seed(args.seed)
  19:     
  20: emb_reader = EmbReader(args.emb_path, emb_dim=args.emb_dim)
  21: emb_words = emb_reader.load_words()
  22: 
  23: dataset.set_score_range(args.data_set)
  24: (train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data((args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path, min_word_freq=args.min_word_freq, emb_words=emb_words)
  25: 
  26: abs_vocab_file = args.abs_out_path + 'vocab.pkl'
  27: with open(abs_vocab_file, 'wb') as vocab_file:
  28:     pk.dump(vocab, vocab_file)
  29: 
  30: train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)
  31: dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)
  32: test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)
  33: 
  34: train_y = np.array(train_y, dtype=K.floatx())
  35: dev_y = np.array(dev_y, dtype=K.floatx())
  36: test_y = np.array(test_y, dtype=K.floatx())
  37: 
  38: if args.prompt_id:
  39:     train_pmt = np.array(train_pmt, dtype='int32')
  40:     dev_pmt = np.array(dev_pmt, dtype='int32')
  41:     test_pmt = np.array(test_pmt, dtype='int32')
  42: 
  43: dev_y_org = dev_y.astype(dataset.get_ref_dtype())
  44: test_y_org = test_y.astype(dataset.get_ref_dtype())
  45: 
  46: train_y = dataset.get_model_friendly_scores(train_y, train_pmt)
  47: dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)
  48: test_y = dataset.get_model_friendly_scores(test_y, test_pmt)
  49: 
  50: 
  51: 
  52: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     
   4:     
   5:     def random_id(size=6, chars=string.ascii_uppercase + string.digits):
   6:         return ''.join(random.choice(chars) for _ in range(size))
   7:     
   8:     ms = int(round(time.time() * 1000))
   9:     rand_seed = ms % (2**32 - 1)
  10:     random.seed(rand_seed)
  11:        
  12:     args = get_args()
  13:     model_id = random_id()
  14:     
  15:     def kappa_metric(t,x):
  16:         u = 0.5 * K.sum(K.square(x - t))
  17:         v = K.dot(K.transpose(x), t - K.mean(t))
  18:         return v / (v + u)
  19:     
  20:     def kappa_loss(t,x):
  21:         u = K.sum(K.square(x - t))
  22:         v = K.dot(K.squeeze(x,1), K.squeeze(t - K.mean(t),1))
  23:         return u / (2*v + u)
  24:     
  25:     lr = space['lr']
  26:     lr = lr*2
  27:     rho = space['rho']
  28:     clipnorm=space['clipnorm']
  29:     eps=1e-6
  30:     
  31:     opt = optimizers.RMSprop(lr=lr,
  32:                             rho=rho,
  33:                             clipnorm=clipnorm,
  34:                             epsilon=eps
  35:                             )
  36:     loss = kappa_loss
  37:     metric = kappa_metric
  38:     dataset.set_score_range(args.data_set)
  39:     evl = Evaluator(dataset, args.prompt_id, args.abs_out_path, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org, model_id=model_id)
  40:     
  41:     abs_vocab_file = args.abs_out_path + 'vocab.pkl'
  42:     with open(abs_vocab_file, 'rb') as vocab_file:
  43:         vocab = pk.load(vocab_file)
  44:     
  45:     train_y_mean = train_y.mean(axis=0)
  46:     if train_y_mean.ndim == 0:
  47:         train_y_mean = np.expand_dims(train_y_mean, axis=1)
  48:     num_outputs = len(train_y_mean)
  49:     
  50:     mask_zero=False
  51:     
  52:     emb_dim = space['emb_dim']
  53:     rnn_dim = space['rnn_dim']
  54:     rnn_dim = int(rnn_dim)
  55:                    
  56:     model = Sequential()
  57:     model.add(Embedding(args.vocab_size, emb_dim, mask_zero=mask_zero))
  58:     model.add(RWA(rnn_dim))
  59:     model.add(Dense(num_outputs))
  60:     if not args.skip_init_bias:
  61:         bias_value = (np.log(train_y_mean) - np.log(1 - train_y_mean)).astype(K.floatx())
  62:         model.layers[-1].bias.set_value(bias_value)
  63:     model.add(Activation('tanh'))
  64:     model.emb_index = 0
  65:     
  66:     emb_path = 'embed/glove.6B.{}d.txt'.format(emb_dim)
  67:     abs_emb_path = args.abs_root + emb_path
  68:     
  69:     emb_reader = EmbReader(abs_emb_path, emb_dim=emb_dim)
  70:     emb_reader.load_embeddings(vocab)
  71:     emb_wts = emb_reader.get_emb_matrix_given_vocab(vocab, model.layers[model.emb_index].get_weights()[0])
  72:     wts = model.layers[model.emb_index].get_weights()
  73:     wts[0] = emb_wts
  74:     model.layers[model.emb_index].set_weights(wts)
  75: 
  76:     model.compile(loss=loss, optimizer=opt, metrics=[metric])
  77:     model_yaml = model.to_yaml()
  78:     print('model_id: %s' % (model_id))
  79:     print(model_yaml)
  80:     print('optimizer:    lr= %.4f, rho= %.4f, clipnorm= %.4f, epsilon= %.4f' % (lr, rho, clipnorm, eps))
  81:     
  82:     print('PARAMS\t\
  83:     %s\t\
  84:     lr= %.4f\t\
  85:     rho= %.4f\t\
  86:     clip= %.4f\t\
  87:     emb= %.4f\t\
  88:     rnn= %.4f' % (model_id, lr, rho, clipnorm, emb_dim, rnn_dim))
  89:     
  90:     for i in range(args.epochs):
  91:         train_history = model.fit(train_x, train_y, batch_size=args.batch_size, epochs=1, verbose=0)
  92:         evl.evaluate(model, i)
  93:         evl.output_info()
  94:         if i>5 and evl.dev_metric<0.4:
  95:             break
  96:         if i>10 and evl.dev_metric<0.5:
  97:             break
  98:         if i>15 and evl.dev_metric<0.6:
  99:             break
 100: 
 101:     best_dev_kappa = evl.best_dev
 102:     best_test_kappa = evl.best_test
 103: 
 104:     print('Test kappa:', best_dev_kappa)
 105:     return {'loss': 1-best_dev_kappa, 'status': STATUS_OK, 'model': model.to_yaml(), 'weights': pk.dumps(model.get_weights())}
 106: 
17/05/22 16:35:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:35:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KB, free 366.3 MB)
17/05/22 16:35:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.2.30:36337 (size: 3.8 KB, free: 366.3 MB)
17/05/22 16:35:17 INFO SparkContext: Created broadcast 0 from broadcast at PythonRDD.scala:482
17/05/22 16:35:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:35:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 114.0 B, free 366.3 MB)
17/05/22 16:35:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.2.30:36337 (size: 114.0 B, free: 366.3 MB)
17/05/22 16:35:17 INFO SparkContext: Created broadcast 1 from broadcast at PythonRDD.scala:482
17/05/22 16:35:18 INFO SparkContext: Starting job: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27
17/05/22 16:35:18 INFO DAGScheduler: Registering RDD 2 (coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:35:18 INFO DAGScheduler: Got job 0 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) with 2 output partitions
17/05/22 16:35:18 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:35:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/05/22 16:35:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/05/22 16:35:18 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents
17/05/22 16:35:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 366.3 MB)
17/05/22 16:35:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KB, free 366.3 MB)
17/05/22 16:35:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.2.30:36337 (size: 3.2 KB, free: 366.3 MB)
17/05/22 16:35:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/05/22 16:35:18 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:35:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/05/22 16:35:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:35:18 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:35:18 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 6462 bytes)
17/05/22 16:35:18 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:35:18 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:35:18 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:35:18 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:35:18 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 6508 bytes)
17/05/22 16:35:18 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/05/22 16:35:18 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/05/22 16:35:18 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/05/22 16:35:18 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/05/22 16:35:18 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/05/22 16:35:18 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/05/22 16:35:18 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/05/22 16:35:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/22 16:35:18 INFO Executor: Fetching file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495485317398
17/05/22 16:35:18 INFO Utils: /home/david/code/python/DeepATS/./elephas/run.py has been previously copied to /tmp/spark-3752d22a-6244-4435-88a3-02348d4672de/userFiles-65cfb66d-8829-4960-bb97-180d78baa697/run.py
17/05/22 16:35:18 INFO PythonRunner: Times: total = 176, boot = 168, init = 8, finish = 0
17/05/22 16:35:18 INFO PythonRunner: Times: total = 191, boot = 180, init = 11, finish = 0
17/05/22 16:35:18 INFO PythonRunner: Times: total = 176, boot = 174, init = 2, finish = 0
17/05/22 16:35:18 INFO PythonRunner: Times: total = 176, boot = 170, init = 6, finish = 0
17/05/22 16:35:18 INFO PythonRunner: Times: total = 193, boot = 183, init = 10, finish = 0
17/05/22 16:35:18 INFO PythonRunner: Times: total = 176, boot = 171, init = 5, finish = 0
17/05/22 16:35:18 INFO PythonRunner: Times: total = 204, boot = 180, init = 23, finish = 1
17/05/22 16:35:18 INFO PythonRunner: Times: total = 176, boot = 173, init = 3, finish = 0
17/05/22 16:35:18 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1969 bytes result sent to driver
17/05/22 16:35:18 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1882 bytes result sent to driver
17/05/22 16:35:18 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1969 bytes result sent to driver
17/05/22 16:35:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1882 bytes result sent to driver
17/05/22 16:35:18 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1969 bytes result sent to driver
17/05/22 16:35:18 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1969 bytes result sent to driver
17/05/22 16:35:18 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1882 bytes result sent to driver
17/05/22 16:35:18 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1969 bytes result sent to driver
17/05/22 16:35:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 440 ms on localhost (executor driver) (1/8)
17/05/22 16:35:18 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 418 ms on localhost (executor driver) (2/8)
17/05/22 16:35:18 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 408 ms on localhost (executor driver) (3/8)
17/05/22 16:35:18 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 410 ms on localhost (executor driver) (4/8)
17/05/22 16:35:18 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 408 ms on localhost (executor driver) (5/8)
17/05/22 16:35:18 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 407 ms on localhost (executor driver) (6/8)
17/05/22 16:35:18 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 417 ms on localhost (executor driver) (7/8)
17/05/22 16:35:18 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 428 ms on localhost (executor driver) (8/8)
17/05/22 16:35:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/05/22 16:35:18 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.466 s
17/05/22 16:35:18 INFO DAGScheduler: looking for newly runnable stages
17/05/22 16:35:18 INFO DAGScheduler: running: Set()
17/05/22 16:35:18 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/05/22 16:35:18 INFO DAGScheduler: failed: Set()
17/05/22 16:35:18 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27), which has no missing parents
17/05/22 16:35:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.0 KB, free 366.3 MB)
17/05/22 16:35:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KB, free 366.3 MB)
17/05/22 16:35:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.2.30:36337 (size: 7.7 KB, free: 366.3 MB)
17/05/22 16:35:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/05/22 16:35:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:35:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/05/22 16:35:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 6176 bytes)
17/05/22 16:35:18 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 6176 bytes)
17/05/22 16:35:18 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
17/05/22 16:35:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
17/05/22 16:35:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:35:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:35:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
17/05/22 16:35:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
Using Theano backend.
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
model_id: PVG0QF
backend: !!python/unicode 'theano'
class_name: Sequential
config:
- class_name: Embedding
  config:
    activity_regularizer: null
    batch_input_shape: !!python/tuple [null, null]
    dtype: int32
    embeddings_constraint: null
    embeddings_initializer:
      class_name: RandomUniform
      config: {maxval: 0.05, minval: -0.05, seed: null}
    embeddings_regularizer: null
    input_dim: 4000
    input_length: null
    mask_zero: false
    name: embedding_1
    output_dim: 100
    trainable: true
- class_name: RWA
  config:
    activation: tanh
    average_constraint: null
    average_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    average_regularizer: null
    bias_constraint: null
    bias_initializer:
      class_name: Zeros
      config: {}
    bias_regularizer: null
    features_constraint: null
    features_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    features_regularizer: null
    go_backwards: false
    implementation: 0
    initial_attention_constraint: null
    initial_attention_initializer:
      class_name: Zeros
      config: {}
    initial_attention_regularizer: null
    name: rwa_1
    recurrent_activation: relu
    recurrent_constraint: null
    recurrent_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    recurrent_regularizer: null
    return_sequences: false
    stateful: false
    trainable: true
    units: 96
    unroll: false
- class_name: Dense
  config:
    activation: linear
    activity_regularizer: null
    bias_constraint: null
    bias_initializer:
      class_name: Zeros
      config: {}
    bias_regularizer: null
    kernel_constraint: null
    kernel_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    kernel_regularizer: null
    name: dense_1
    trainable: true
    units: 1
    use_bias: true
- class_name: Activation
  config: {activation: tanh, name: activation_1, trainable: true}
keras_version: 2.0.4

optimizer:    lr= 0.0026, rho= 0.8574, clipnorm= 1.4187, epsilon= 0.0000
PARAMS	    PVG0QF	    lr= 0.0026	    rho= 0.8574	    clip= 1.4187	    emb= 100.0000	    rnn= 96.0000
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/05/22 16:38:10 WARN Utils: Your hostname, dvaughn-linux resolves to a loopback address: 127.0.1.1; using 172.16.2.30 instead (on interface eno1)
17/05/22 16:38:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
17/05/22 16:38:15 INFO SparkContext: Running Spark version 2.1.0
17/05/22 16:38:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/22 16:38:15 INFO SecurityManager: Changing view acls to: root
17/05/22 16:38:15 INFO SecurityManager: Changing modify acls to: root
17/05/22 16:38:15 INFO SecurityManager: Changing view acls groups to: 
17/05/22 16:38:15 INFO SecurityManager: Changing modify acls groups to: 
17/05/22 16:38:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/05/22 16:38:16 INFO Utils: Successfully started service 'sparkDriver' on port 45179.
17/05/22 16:38:16 INFO SparkEnv: Registering MapOutputTracker
17/05/22 16:38:16 INFO SparkEnv: Registering BlockManagerMaster
17/05/22 16:38:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/22 16:38:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/22 16:38:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ca9c495c-93fb-45c2-8d54-ba73dcf61cb0
17/05/22 16:38:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/05/22 16:38:16 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/22 16:38:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/22 16:38:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.2.30:4040
17/05/22 16:38:16 INFO SparkContext: Added file file:/home/david/code/python/DeepATS/./elephas/run.py at file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495485496480
17/05/22 16:38:16 INFO Utils: Copying /home/david/code/python/DeepATS/elephas/run.py to /tmp/spark-d7c8d170-ec07-43bb-93bb-92aa6a67088b/userFiles-ace422c3-d169-4f10-bd0e-8eaf3dc95282/run.py
17/05/22 16:38:16 INFO Executor: Starting executor ID driver on host localhost
17/05/22 16:38:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45895.
17/05/22 16:38:16 INFO NettyBlockTransferService: Server created on 172.16.2.30:45895
17/05/22 16:38:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/22 16:38:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.30, 45895, None)
17/05/22 16:38:16 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.30:45895 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.30, 45895, None)
17/05/22 16:38:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.30, 45895, None)
17/05/22 16:38:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.30, 45895, None)
>>> Hyperas search space:

def get_space():
    return {
        'lr': hp.lognormal('lr', -3 * 2.3, .8),
        'rho': hp.normal('rho', .875, .04),
        'clipnorm': hp.uniform('clipnorm', 1, 15),
        'emb_dim': hp.choice('emb_dim', [50, 100, 200, 300]),
        'rnn_dim': hp.uniform('rnn_dim', 50, 300),
    }

>>> Data
   1: 
   2: from keras.utils import np_utils
   3: from keras.preprocessing import sequence
   4: import keras.backend as K
   5: import numpy as np
   6: 
   7: import pickle as pk
   8: import deepats.asap_reader as dataset
   9: from deepats.w2vEmbReader import W2VEmbReader as EmbReader
  10: from deepats.config import get_args
  11: 
  12: import logging
  13: logger = logging.getLogger(__name__)
  14: 
  15: args = get_args()
  16: 
  17: if args.seed > 0:
  18:     np.random.seed(args.seed)
  19:     
  20: emb_reader = EmbReader(args.emb_path, emb_dim=args.emb_dim)
  21: emb_words = emb_reader.load_words()
  22: 
  23: dataset.set_score_range(args.data_set)
  24: (train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data((args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path, min_word_freq=args.min_word_freq, emb_words=emb_words)
  25: 
  26: abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  27: with open(abs_vocab_file, 'wb') as vocab_file:
  28:     pk.dump(vocab, vocab_file)
  29: 
  30: train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)
  31: dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)
  32: test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)
  33: 
  34: train_y = np.array(train_y, dtype=K.floatx())
  35: dev_y = np.array(dev_y, dtype=K.floatx())
  36: test_y = np.array(test_y, dtype=K.floatx())
  37: 
  38: if args.prompt_id:
  39:     train_pmt = np.array(train_pmt, dtype='int32')
  40:     dev_pmt = np.array(dev_pmt, dtype='int32')
  41:     test_pmt = np.array(test_pmt, dtype='int32')
  42: 
  43: dev_y_org = dev_y.astype(dataset.get_ref_dtype())
  44: test_y_org = test_y.astype(dataset.get_ref_dtype())
  45: 
  46: train_y = dataset.get_model_friendly_scores(train_y, train_pmt)
  47: dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)
  48: test_y = dataset.get_model_friendly_scores(test_y, test_pmt)
  49: 
  50: 
  51: 
  52: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     
   4:     
   5:     def random_id(size=6, chars=string.ascii_uppercase + string.digits):
   6:         return ''.join(random.choice(chars) for _ in range(size))
   7:     
   8:     ms = int(round(time.time() * 1000))
   9:     rand_seed = ms % (2**32 - 1)
  10:     random.seed(rand_seed)
  11:        
  12:     args = get_args()
  13:     model_id = random_id()
  14:     
  15:     def kappa_metric(t,x):
  16:         u = 0.5 * K.sum(K.square(x - t))
  17:         v = K.dot(K.transpose(x), t - K.mean(t))
  18:         return v / (v + u)
  19:     
  20:     def kappa_loss(t,x):
  21:         u = K.sum(K.square(x - t))
  22:         v = K.dot(K.squeeze(x,1), K.squeeze(t - K.mean(t),1))
  23:         return u / (2*v + u)
  24:     
  25:     lr = space['lr']
  26:     lr = lr*2
  27:     rho = space['rho']
  28:     clipnorm=space['clipnorm']
  29:     eps=1e-6
  30:     
  31:     opt = optimizers.RMSprop(lr=lr,
  32:                             rho=rho,
  33:                             clipnorm=clipnorm,
  34:                             epsilon=eps
  35:                             )
  36:     loss = kappa_loss
  37:     metric = kappa_metric
  38:     dataset.set_score_range(args.data_set)
  39:     evl = Evaluator(dataset, args.prompt_id, args.abs_out_path, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org, model_id=model_id)
  40:     
  41:     abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  42:     with open(abs_vocab_file, 'rb') as vocab_file:
  43:         vocab = pk.load(vocab_file)
  44:     
  45:     train_y_mean = train_y.mean(axis=0)
  46:     if train_y_mean.ndim == 0:
  47:         train_y_mean = np.expand_dims(train_y_mean, axis=1)
  48:     num_outputs = len(train_y_mean)
  49:     
  50:     mask_zero=False
  51:     
  52:     emb_dim = space['emb_dim']
  53:     rnn_dim = space['rnn_dim']
  54:     rnn_dim = int(rnn_dim)
  55:                    
  56:     model = Sequential()
  57:     model.add(Embedding(args.vocab_size, emb_dim, mask_zero=mask_zero))
  58:     model.add(RWA(rnn_dim))
  59:     model.add(Dense(num_outputs))
  60:     if not args.skip_init_bias:
  61:         bias_value = (np.log(train_y_mean) - np.log(1 - train_y_mean)).astype(K.floatx())
  62:         model.layers[-1].bias.set_value(bias_value)
  63:     model.add(Activation('tanh'))
  64:     model.emb_index = 0
  65:     
  66:     emb_path = 'embed/glove.6B.{}d.txt'.format(emb_dim)
  67:     abs_emb_path = args.abs_root + emb_path
  68:     
  69:     emb_reader = EmbReader(abs_emb_path, emb_dim=emb_dim)
  70:     emb_reader.load_embeddings(vocab)
  71:     emb_wts = emb_reader.get_emb_matrix_given_vocab(vocab, model.layers[model.emb_index].get_weights()[0])
  72:     wts = model.layers[model.emb_index].get_weights()
  73:     wts[0] = emb_wts
  74:     model.layers[model.emb_index].set_weights(wts)
  75: 
  76:     model.compile(loss=loss, optimizer=opt, metrics=[metric])
  77:     model_yaml = model.to_yaml()
  78:     print('model_id: %s' % (model_id))
  79:     print(model_yaml)
  80:     print('optimizer:    lr= %.4f, rho= %.4f, clipnorm= %.4f, epsilon= %.4f' % (lr, rho, clipnorm, eps))
  81:     
  82:     print('PARAMS\t\
  83:     %s\t\
  84:     lr= %.4f\t\
  85:     rho= %.4f\t\
  86:     clip= %.4f\t\
  87:     emb= %.4f\t\
  88:     rnn= %.4f' % (model_id, lr, rho, clipnorm, emb_dim, rnn_dim))
  89:     
  90:     for i in range(args.epochs):
  91:         train_history = model.fit(train_x, train_y, batch_size=args.batch_size, epochs=1, verbose=0)
  92:         evl.evaluate(model, i)
  93:         evl.output_info()
  94:         if i>5 and evl.dev_metric<0.4:
  95:             break
  96:         if i>10 and evl.dev_metric<0.5:
  97:             break
  98:         if i>15 and evl.dev_metric<0.6:
  99:             break
 100: 
 101:     best_dev_kappa = evl.best_dev
 102:     best_test_kappa = evl.best_test
 103: 
 104:     print('Test kappa:', best_dev_kappa)
 105:     return {'loss': 1-best_dev_kappa, 'status': STATUS_OK, 'model': model.to_yaml(), 'weights': pk.dumps(model.get_weights())}
 106: 
17/05/22 16:38:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:38:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KB, free 366.3 MB)
17/05/22 16:38:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.2.30:45895 (size: 3.8 KB, free: 366.3 MB)
17/05/22 16:38:16 INFO SparkContext: Created broadcast 0 from broadcast at PythonRDD.scala:482
17/05/22 16:38:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:38:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 114.0 B, free 366.3 MB)
17/05/22 16:38:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.2.30:45895 (size: 114.0 B, free: 366.3 MB)
17/05/22 16:38:16 INFO SparkContext: Created broadcast 1 from broadcast at PythonRDD.scala:482
17/05/22 16:38:17 INFO SparkContext: Starting job: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27
17/05/22 16:38:17 INFO DAGScheduler: Registering RDD 2 (coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:38:17 INFO DAGScheduler: Got job 0 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) with 2 output partitions
17/05/22 16:38:17 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:38:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/05/22 16:38:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/05/22 16:38:17 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents
17/05/22 16:38:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 366.3 MB)
17/05/22 16:38:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KB, free 366.3 MB)
17/05/22 16:38:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.2.30:45895 (size: 3.2 KB, free: 366.3 MB)
17/05/22 16:38:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/05/22 16:38:17 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:38:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/05/22 16:38:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:38:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:38:17 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 6462 bytes)
17/05/22 16:38:17 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:38:17 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:38:17 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:38:17 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:38:17 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 6508 bytes)
17/05/22 16:38:17 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/05/22 16:38:17 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/05/22 16:38:17 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/05/22 16:38:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/22 16:38:17 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/05/22 16:38:17 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/05/22 16:38:17 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/05/22 16:38:17 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/05/22 16:38:17 INFO Executor: Fetching file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495485496480
17/05/22 16:38:17 INFO Utils: /home/david/code/python/DeepATS/./elephas/run.py has been previously copied to /tmp/spark-d7c8d170-ec07-43bb-93bb-92aa6a67088b/userFiles-ace422c3-d169-4f10-bd0e-8eaf3dc95282/run.py
17/05/22 16:38:17 INFO PythonRunner: Times: total = 216, boot = 211, init = 5, finish = 0
17/05/22 16:38:17 INFO PythonRunner: Times: total = 213, boot = 194, init = 19, finish = 0
17/05/22 16:38:17 INFO PythonRunner: Times: total = 201, boot = 191, init = 10, finish = 0
17/05/22 16:38:17 INFO PythonRunner: Times: total = 213, boot = 199, init = 14, finish = 0
17/05/22 16:38:17 INFO PythonRunner: Times: total = 213, boot = 198, init = 15, finish = 0
17/05/22 16:38:17 INFO PythonRunner: Times: total = 200, boot = 192, init = 8, finish = 0
17/05/22 16:38:17 INFO PythonRunner: Times: total = 196, boot = 189, init = 7, finish = 0
17/05/22 16:38:17 INFO PythonRunner: Times: total = 213, boot = 196, init = 17, finish = 0
17/05/22 16:38:17 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1969 bytes result sent to driver
17/05/22 16:38:17 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1882 bytes result sent to driver
17/05/22 16:38:17 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1969 bytes result sent to driver
17/05/22 16:38:17 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1969 bytes result sent to driver
17/05/22 16:38:17 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1882 bytes result sent to driver
17/05/22 16:38:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1969 bytes result sent to driver
17/05/22 16:38:17 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1969 bytes result sent to driver
17/05/22 16:38:17 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1882 bytes result sent to driver
17/05/22 16:38:17 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 454 ms on localhost (executor driver) (1/8)
17/05/22 16:38:17 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 454 ms on localhost (executor driver) (2/8)
17/05/22 16:38:17 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 449 ms on localhost (executor driver) (3/8)
17/05/22 16:38:17 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 452 ms on localhost (executor driver) (4/8)
17/05/22 16:38:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 480 ms on localhost (executor driver) (5/8)
17/05/22 16:38:17 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 448 ms on localhost (executor driver) (6/8)
17/05/22 16:38:17 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 450 ms on localhost (executor driver) (7/8)
17/05/22 16:38:17 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 455 ms on localhost (executor driver) (8/8)
17/05/22 16:38:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/05/22 16:38:17 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.497 s
17/05/22 16:38:17 INFO DAGScheduler: looking for newly runnable stages
17/05/22 16:38:17 INFO DAGScheduler: running: Set()
17/05/22 16:38:17 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/05/22 16:38:17 INFO DAGScheduler: failed: Set()
17/05/22 16:38:17 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27), which has no missing parents
17/05/22 16:38:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.1 KB, free 366.3 MB)
17/05/22 16:38:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KB, free 366.3 MB)
17/05/22 16:38:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.2.30:45895 (size: 7.7 KB, free: 366.3 MB)
17/05/22 16:38:17 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/05/22 16:38:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:38:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/05/22 16:38:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 6176 bytes)
17/05/22 16:38:17 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 6176 bytes)
17/05/22 16:38:17 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
17/05/22 16:38:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
17/05/22 16:38:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:38:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:38:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
17/05/22 16:38:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
Using Theano backend.
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
model_id: ZCEKRY
backend: !!python/unicode 'theano'
class_name: Sequential
config:
- class_name: Embedding
  config:
    activity_regularizer: null
    batch_input_shape: !!python/tuple [null, null]
    dtype: int32
    embeddings_constraint: null
    embeddings_initializer:
      class_name: RandomUniform
      config: {maxval: 0.05, minval: -0.05, seed: null}
    embeddings_regularizer: null
    input_dim: 4000
    input_length: null
    mask_zero: false
    name: embedding_1
    output_dim: 50
    trainable: true
- class_name: RWA
  config:
    activation: tanh
    average_constraint: null
    average_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    average_regularizer: null
    bias_constraint: null
    bias_initializer:
      class_name: Zeros
      config: {}
    bias_regularizer: null
    features_constraint: null
    features_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    features_regularizer: null
    go_backwards: false
    implementation: 0
    initial_attention_constraint: null
    initial_attention_initializer:
      class_name: Zeros
      config: {}
    initial_attention_regularizer: null
    name: rwa_1
    recurrent_activation: relu
    recurrent_constraint: null
    recurrent_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    recurrent_regularizer: null
    return_sequences: false
    stateful: false
    trainable: true
    units: 74
    unroll: false
- class_name: Dense
  config:
    activation: linear
    activity_regularizer: null
    bias_constraint: null
    bias_initializer:
      class_name: Zeros
      config: {}
    bias_regularizer: null
    kernel_constraint: null
    kernel_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    kernel_regularizer: null
    name: dense_1
    trainable: true
    units: 1
    use_bias: true
- class_name: Activation
  config: {activation: tanh, name: activation_1, trainable: true}
keras_version: 2.0.4

optimizer:    lr= 0.0023, rho= 0.8733, clipnorm= 10.7905, epsilon= 0.0000
PARAMS	    ZCEKRY	    lr= 0.0023	    rho= 0.8733	    clip= 10.7905	    emb= 50.0000	    rnn= 74.0000
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/05/22 16:39:06 WARN Utils: Your hostname, dvaughn-linux resolves to a loopback address: 127.0.1.1; using 172.16.2.30 instead (on interface eno1)
17/05/22 16:39:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
17/05/22 16:39:11 INFO SparkContext: Running Spark version 2.1.0
17/05/22 16:39:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/22 16:39:12 INFO SecurityManager: Changing view acls to: root
17/05/22 16:39:12 INFO SecurityManager: Changing modify acls to: root
17/05/22 16:39:12 INFO SecurityManager: Changing view acls groups to: 
17/05/22 16:39:12 INFO SecurityManager: Changing modify acls groups to: 
17/05/22 16:39:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/05/22 16:39:12 INFO Utils: Successfully started service 'sparkDriver' on port 42392.
17/05/22 16:39:12 INFO SparkEnv: Registering MapOutputTracker
17/05/22 16:39:12 INFO SparkEnv: Registering BlockManagerMaster
17/05/22 16:39:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/22 16:39:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/22 16:39:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5a4af5c9-c787-47f9-a7cb-e87f28ebcb0a
17/05/22 16:39:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/05/22 16:39:12 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/22 16:39:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/22 16:39:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.2.30:4040
17/05/22 16:39:12 INFO SparkContext: Added file file:/home/david/code/python/DeepATS/./elephas/run.py at file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495485552639
17/05/22 16:39:12 INFO Utils: Copying /home/david/code/python/DeepATS/elephas/run.py to /tmp/spark-4011db6f-3859-49d2-9ca2-d9045d0fa0b2/userFiles-a83d77c3-2b30-4738-8e3f-b49bb40760ba/run.py
17/05/22 16:39:12 INFO Executor: Starting executor ID driver on host localhost
17/05/22 16:39:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38767.
17/05/22 16:39:12 INFO NettyBlockTransferService: Server created on 172.16.2.30:38767
17/05/22 16:39:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/22 16:39:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.30, 38767, None)
17/05/22 16:39:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.30:38767 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.30, 38767, None)
17/05/22 16:39:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.30, 38767, None)
17/05/22 16:39:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.30, 38767, None)
>>> Hyperas search space:

def get_space():
    return {
        'lr': hp.lognormal('lr', -3 * 2.3, .8),
        'rho': hp.normal('rho', .875, .04),
        'clipnorm': hp.uniform('clipnorm', 1, 15),
        'emb_dim': hp.choice('emb_dim', [50, 100, 200, 300]),
        'rnn_dim': hp.uniform('rnn_dim', 50, 300),
    }

>>> Data
   1: 
   2: from keras.utils import np_utils
   3: from keras.preprocessing import sequence
   4: import keras.backend as K
   5: import numpy as np
   6: 
   7: import pickle as pk
   8: import deepats.asap_reader as dataset
   9: from deepats.w2vEmbReader import W2VEmbReader as EmbReader
  10: from deepats.config import get_args
  11: 
  12: import logging
  13: logger = logging.getLogger(__name__)
  14: 
  15: args = get_args()
  16: 
  17: if args.seed > 0:
  18:     np.random.seed(args.seed)
  19:     
  20: emb_reader = EmbReader(args.emb_path, emb_dim=args.emb_dim)
  21: emb_words = emb_reader.load_words()
  22: 
  23: dataset.set_score_range(args.data_set)
  24: (train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data((args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path, min_word_freq=args.min_word_freq, emb_words=emb_words)
  25: 
  26: abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  27: with open(abs_vocab_file, 'wb') as vocab_file:
  28:     pk.dump(vocab, vocab_file)
  29: 
  30: train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)
  31: dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)
  32: test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)
  33: 
  34: train_y = np.array(train_y, dtype=K.floatx())
  35: dev_y = np.array(dev_y, dtype=K.floatx())
  36: test_y = np.array(test_y, dtype=K.floatx())
  37: 
  38: if args.prompt_id:
  39:     train_pmt = np.array(train_pmt, dtype='int32')
  40:     dev_pmt = np.array(dev_pmt, dtype='int32')
  41:     test_pmt = np.array(test_pmt, dtype='int32')
  42: 
  43: dev_y_org = dev_y.astype(dataset.get_ref_dtype())
  44: test_y_org = test_y.astype(dataset.get_ref_dtype())
  45: 
  46: train_y = dataset.get_model_friendly_scores(train_y, train_pmt)
  47: dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)
  48: test_y = dataset.get_model_friendly_scores(test_y, test_pmt)
  49: 
  50: 
  51: 
  52: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     
   4:     
   5:     def random_id(size=6, chars=string.ascii_uppercase + string.digits):
   6:         return ''.join(random.choice(chars) for _ in range(size))
   7:     
   8:     ms = int(round(time.time() * 1000))
   9:     rand_seed = ms % (2**32 - 1)
  10:     random.seed(rand_seed)
  11:        
  12:     args = get_args()
  13:     model_id = random_id()
  14:     
  15:     def kappa_metric(t,x):
  16:         u = 0.5 * K.sum(K.square(x - t))
  17:         v = K.dot(K.transpose(x), t - K.mean(t))
  18:         return v / (v + u)
  19:     
  20:     def kappa_loss(t,x):
  21:         u = K.sum(K.square(x - t))
  22:         v = K.dot(K.squeeze(x,1), K.squeeze(t - K.mean(t),1))
  23:         return u / (2*v + u)
  24:     
  25:     lr = space['lr']
  26:     lr = lr*2
  27:     rho = space['rho']
  28:     clipnorm=space['clipnorm']
  29:     eps=1e-6
  30:     
  31:     opt = optimizers.RMSprop(lr=lr,
  32:                             rho=rho,
  33:                             clipnorm=clipnorm,
  34:                             epsilon=eps
  35:                             )
  36:     loss = kappa_loss
  37:     metric = kappa_metric
  38:     dataset.set_score_range(args.data_set)
  39:     evl = Evaluator(dataset, args.prompt_id, args.abs_out_path, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org, model_id=model_id)
  40:     
  41:     abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  42:     with open(abs_vocab_file, 'rb') as vocab_file:
  43:         vocab = pk.load(vocab_file)
  44:     
  45:     train_y_mean = train_y.mean(axis=0)
  46:     if train_y_mean.ndim == 0:
  47:         train_y_mean = np.expand_dims(train_y_mean, axis=1)
  48:     num_outputs = len(train_y_mean)
  49:     
  50:     mask_zero=False
  51:     
  52:     emb_dim = space['emb_dim']
  53:     rnn_dim = space['rnn_dim']
  54:     rnn_dim = int(rnn_dim)
  55:                    
  56:     model = Sequential()
  57:     model.add(Embedding(args.vocab_size, emb_dim, mask_zero=mask_zero))
  58:     model.add(RWA(rnn_dim))
  59:     model.add(Dense(num_outputs))
  60:     if not args.skip_init_bias:
  61:         bias_value = (np.log(train_y_mean) - np.log(1 - train_y_mean)).astype(K.floatx())
  62:         model.layers[-1].bias.set_value(bias_value)
  63:     model.add(Activation('tanh'))
  64:     model.emb_index = 0
  65:     
  66:     emb_path = 'embed/glove.6B.{}d.txt'.format(emb_dim)
  67:     abs_emb_path = args.abs_root + emb_path
  68:     
  69:     emb_reader = EmbReader(abs_emb_path, emb_dim=emb_dim)
  70:     emb_reader.load_embeddings(vocab)
  71:     emb_wts = emb_reader.get_emb_matrix_given_vocab(vocab, model.layers[model.emb_index].get_weights()[0])
  72:     wts = model.layers[model.emb_index].get_weights()
  73:     wts[0] = emb_wts
  74:     model.layers[model.emb_index].set_weights(wts)
  75: 
  76:     model.compile(loss=loss, optimizer=opt, metrics=[metric])
  77:     model_yaml = model.to_yaml()
  78:     print('model_id: %s' % (model_id))
  79:     print(model_yaml)
  80:     print('optimizer:    lr= %.4f, rho= %.4f, clipnorm= %.4f, epsilon= %.4f' % (lr, rho, clipnorm, eps))
  81:     
  82:     print('PARAMS\t\
  83:     %s\t\
  84:     lr= %.4f\t\
  85:     rho= %.4f\t\
  86:     clip= %.4f\t\
  87:     emb= %.4f\t\
  88:     rnn= %.4f' % (model_id, lr, rho, clipnorm, emb_dim, rnn_dim))
  89:     
  90:     for i in range(args.epochs):
  91:         train_history = model.fit(train_x, train_y, batch_size=args.batch_size, epochs=1, verbose=0)
  92:         evl.evaluate(model, i)
  93:         evl.output_info()
  94:         if i>5 and evl.dev_metric<0.4:
  95:             break
  96:         if i>10 and evl.dev_metric<0.5:
  97:             break
  98:         if i>15 and evl.dev_metric<0.6:
  99:             break
 100: 
 101:     best_dev_kappa = evl.best_dev
 102:     best_test_kappa = evl.best_test
 103: 
 104:     print('Test kappa:', best_dev_kappa)
 105:     return {'loss': 1-best_dev_kappa, 'status': STATUS_OK, 'model': model.to_yaml(), 'weights': pk.dumps(model.get_weights())}
 106: 
17/05/22 16:39:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:39:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KB, free 366.3 MB)
17/05/22 16:39:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.2.30:38767 (size: 3.8 KB, free: 366.3 MB)
17/05/22 16:39:13 INFO SparkContext: Created broadcast 0 from broadcast at PythonRDD.scala:482
17/05/22 16:39:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:39:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 114.0 B, free 366.3 MB)
17/05/22 16:39:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.2.30:38767 (size: 114.0 B, free: 366.3 MB)
17/05/22 16:39:13 INFO SparkContext: Created broadcast 1 from broadcast at PythonRDD.scala:482
17/05/22 16:39:13 INFO SparkContext: Starting job: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27
17/05/22 16:39:13 INFO DAGScheduler: Registering RDD 2 (coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:39:13 INFO DAGScheduler: Got job 0 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) with 2 output partitions
17/05/22 16:39:13 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:39:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/05/22 16:39:13 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/05/22 16:39:13 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents
17/05/22 16:39:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 366.3 MB)
17/05/22 16:39:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KB, free 366.3 MB)
17/05/22 16:39:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.2.30:38767 (size: 3.2 KB, free: 366.3 MB)
17/05/22 16:39:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/05/22 16:39:13 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:39:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/05/22 16:39:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:39:13 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:39:13 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 6462 bytes)
17/05/22 16:39:13 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:39:13 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:39:13 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:39:13 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:39:13 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 6508 bytes)
17/05/22 16:39:13 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/05/22 16:39:13 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/05/22 16:39:13 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/05/22 16:39:13 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/05/22 16:39:13 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/05/22 16:39:13 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/05/22 16:39:13 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/05/22 16:39:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/22 16:39:13 INFO Executor: Fetching file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495485552639
17/05/22 16:39:13 INFO Utils: /home/david/code/python/DeepATS/./elephas/run.py has been previously copied to /tmp/spark-4011db6f-3859-49d2-9ca2-d9045d0fa0b2/userFiles-a83d77c3-2b30-4738-8e3f-b49bb40760ba/run.py
17/05/22 16:39:13 INFO PythonRunner: Times: total = 179, boot = 173, init = 6, finish = 0
17/05/22 16:39:13 INFO PythonRunner: Times: total = 195, boot = 179, init = 16, finish = 0
17/05/22 16:39:13 INFO PythonRunner: Times: total = 196, boot = 182, init = 14, finish = 0
17/05/22 16:39:13 INFO PythonRunner: Times: total = 173, boot = 166, init = 6, finish = 1
17/05/22 16:39:13 INFO PythonRunner: Times: total = 180, boot = 176, init = 3, finish = 1
17/05/22 16:39:13 INFO PythonRunner: Times: total = 179, boot = 175, init = 3, finish = 1
17/05/22 16:39:13 INFO PythonRunner: Times: total = 180, boot = 171, init = 9, finish = 0
17/05/22 16:39:13 INFO PythonRunner: Times: total = 199, boot = 178, init = 20, finish = 1
17/05/22 16:39:13 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1969 bytes result sent to driver
17/05/22 16:39:13 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1969 bytes result sent to driver
17/05/22 16:39:13 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1969 bytes result sent to driver
17/05/22 16:39:13 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1969 bytes result sent to driver
17/05/22 16:39:13 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1969 bytes result sent to driver
17/05/22 16:39:13 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1969 bytes result sent to driver
17/05/22 16:39:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1969 bytes result sent to driver
17/05/22 16:39:13 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1969 bytes result sent to driver
17/05/22 16:39:13 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 422 ms on localhost (executor driver) (1/8)
17/05/22 16:39:13 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 422 ms on localhost (executor driver) (2/8)
17/05/22 16:39:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 462 ms on localhost (executor driver) (3/8)
17/05/22 16:39:13 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 421 ms on localhost (executor driver) (4/8)
17/05/22 16:39:13 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 420 ms on localhost (executor driver) (5/8)
17/05/22 16:39:13 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 425 ms on localhost (executor driver) (6/8)
17/05/22 16:39:13 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 432 ms on localhost (executor driver) (7/8)
17/05/22 16:39:13 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 430 ms on localhost (executor driver) (8/8)
17/05/22 16:39:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/05/22 16:39:13 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.481 s
17/05/22 16:39:13 INFO DAGScheduler: looking for newly runnable stages
17/05/22 16:39:13 INFO DAGScheduler: running: Set()
17/05/22 16:39:13 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/05/22 16:39:13 INFO DAGScheduler: failed: Set()
17/05/22 16:39:13 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27), which has no missing parents
17/05/22 16:39:13 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.1 KB, free 366.3 MB)
17/05/22 16:39:13 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KB, free 366.3 MB)
17/05/22 16:39:13 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.2.30:38767 (size: 7.7 KB, free: 366.3 MB)
17/05/22 16:39:13 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/05/22 16:39:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:39:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/05/22 16:39:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 6176 bytes)
17/05/22 16:39:13 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 6176 bytes)
17/05/22 16:39:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
17/05/22 16:39:13 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
17/05/22 16:39:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:39:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:39:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
17/05/22 16:39:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
17/05/22 16:39:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.16.2.30:38767 in memory (size: 3.2 KB, free: 366.3 MB)
Using Theano backend.
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
17/05/22 16:39:25 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 8)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/05/22 16:39:25 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

17/05/22 16:39:25 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
17/05/22 16:39:25 INFO TaskSchedulerImpl: Cancelling stage 1
17/05/22 16:39:25 INFO Executor: Executor is trying to kill task 1.0 in stage 1.0 (TID 9)
17/05/22 16:39:25 INFO TaskSchedulerImpl: Stage 1 was cancelled
17/05/22 16:39:25 INFO DAGScheduler: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) failed in 11.899 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
17/05/22 16:39:25 INFO DAGScheduler: Job 0 failed: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27, took 12.456872 s
Traceback (most recent call last):
  File "/home/david/code/python/DeepATS/./elephas/run.py", line 190, in <module>
    best_model = hyperparam_model.minimize(model=model, data=data, max_evals=100)
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 32, in minimize
    trials_list = self.compute_trials(model, data, max_evals)
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 27, in compute_trials
    trials_list = dummy_rdd.mapPartitions(hyperas_worker.minimize).collect()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in collect
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:934)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

17/05/22 16:39:25 WARN PythonRunner: Incomplete task interrupted: Attempting to kill Python Worker
17/05/22 16:39:25 INFO Executor: Executor killed task 1.0 in stage 1.0 (TID 9)
17/05/22 16:39:25 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 9, localhost, executor driver): TaskKilled (killed intentionally)
17/05/22 16:39:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/05/22 16:39:26 INFO SparkContext: Invoking stop() from shutdown hook
17/05/22 16:39:26 INFO SparkUI: Stopped Spark web UI at http://172.16.2.30:4040
17/05/22 16:39:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/05/22 16:39:26 INFO MemoryStore: MemoryStore cleared
17/05/22 16:39:26 INFO BlockManager: BlockManager stopped
17/05/22 16:39:26 INFO BlockManagerMaster: BlockManagerMaster stopped
17/05/22 16:39:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/05/22 16:39:26 INFO SparkContext: Successfully stopped SparkContext
17/05/22 16:39:26 INFO ShutdownHookManager: Shutdown hook called
17/05/22 16:39:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-4011db6f-3859-49d2-9ca2-d9045d0fa0b2/pyspark-0074bee1-4b48-4195-9f3b-fdc74acb658c
17/05/22 16:39:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-4011db6f-3859-49d2-9ca2-d9045d0fa0b2
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/05/22 16:45:59 WARN Utils: Your hostname, dvaughn-linux resolves to a loopback address: 127.0.1.1; using 172.16.2.30 instead (on interface eno1)
17/05/22 16:45:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
17/05/22 16:46:05 INFO SparkContext: Running Spark version 2.1.0
17/05/22 16:46:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/22 16:46:05 INFO SecurityManager: Changing view acls to: root
17/05/22 16:46:05 INFO SecurityManager: Changing modify acls to: root
17/05/22 16:46:05 INFO SecurityManager: Changing view acls groups to: 
17/05/22 16:46:05 INFO SecurityManager: Changing modify acls groups to: 
17/05/22 16:46:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/05/22 16:46:05 INFO Utils: Successfully started service 'sparkDriver' on port 46448.
17/05/22 16:46:05 INFO SparkEnv: Registering MapOutputTracker
17/05/22 16:46:05 INFO SparkEnv: Registering BlockManagerMaster
17/05/22 16:46:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/22 16:46:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/22 16:46:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-780f3fd8-73ff-4e25-ae2f-fd393ed62c46
17/05/22 16:46:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/05/22 16:46:06 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/22 16:46:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/22 16:46:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.2.30:4040
17/05/22 16:46:06 INFO SparkContext: Added file file:/home/david/code/python/DeepATS/./elephas/run.py at file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495485966341
17/05/22 16:46:06 INFO Utils: Copying /home/david/code/python/DeepATS/elephas/run.py to /tmp/spark-0a1a159b-b2d2-45f0-ae22-98eb2384b53d/userFiles-786210f3-a364-4ff7-81f6-850b0648d8d7/run.py
17/05/22 16:46:06 INFO Executor: Starting executor ID driver on host localhost
17/05/22 16:46:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37218.
17/05/22 16:46:06 INFO NettyBlockTransferService: Server created on 172.16.2.30:37218
17/05/22 16:46:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/22 16:46:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.30, 37218, None)
17/05/22 16:46:06 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.30:37218 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.30, 37218, None)
17/05/22 16:46:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.30, 37218, None)
17/05/22 16:46:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.30, 37218, None)
>>> Hyperas search space:

def get_space():
    return {
        'lr': hp.lognormal('lr', -3 * 2.3, .8),
        'rho': hp.normal('rho', .875, .04),
        'clipnorm': hp.uniform('clipnorm', 1, 15),
        'emb_dim': hp.choice('emb_dim', [50, 100, 200, 300]),
        'rnn_dim': hp.uniform('rnn_dim', 50, 300),
    }

>>> Data
   1: 
   2: from keras.utils import np_utils
   3: from keras.preprocessing import sequence
   4: import keras.backend as K
   5: import numpy as np
   6: 
   7: import pickle as pk
   8: import deepats.asap_reader as dataset
   9: from deepats.w2vEmbReader import W2VEmbReader as EmbReader
  10: from deepats.config import get_args
  11: 
  12: import logging
  13: logger = logging.getLogger(__name__)
  14: 
  15: args = get_args()
  16: 
  17: if args.seed > 0:
  18:     np.random.seed(args.seed)
  19:     
  20: emb_reader = EmbReader(args.emb_path, emb_dim=args.emb_dim)
  21: emb_words = emb_reader.load_words()
  22: 
  23: dataset.set_score_range(args.data_set)
  24: (train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data((args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path, min_word_freq=args.min_word_freq, emb_words=emb_words)
  25: 
  26: abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  27: with open(abs_vocab_file, 'wb') as vocab_file:
  28:     pk.dump(vocab, vocab_file)
  29: 
  30: train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)
  31: dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)
  32: test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)
  33: 
  34: train_y = np.array(train_y, dtype=K.floatx())
  35: dev_y = np.array(dev_y, dtype=K.floatx())
  36: test_y = np.array(test_y, dtype=K.floatx())
  37: 
  38: if args.prompt_id:
  39:     train_pmt = np.array(train_pmt, dtype='int32')
  40:     dev_pmt = np.array(dev_pmt, dtype='int32')
  41:     test_pmt = np.array(test_pmt, dtype='int32')
  42: 
  43: dev_y_org = dev_y.astype(dataset.get_ref_dtype())
  44: test_y_org = test_y.astype(dataset.get_ref_dtype())
  45: 
  46: train_y = dataset.get_model_friendly_scores(train_y, train_pmt)
  47: dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)
  48: test_y = dataset.get_model_friendly_scores(test_y, test_pmt)
  49: 
  50: 
  51: 
  52: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     
   4:     
   5:     def random_id(size=6, chars=string.ascii_uppercase + string.digits):
   6:         return ''.join(random.choice(chars) for _ in range(size))
   7:     
   8:     ms = int(round(time.time() * 1000))
   9:     rand_seed = ms % (2**32 - 1)
  10:     random.seed(rand_seed)
  11:        
  12:     args = get_args()
  13:     model_id = random_id()
  14:     
  15:     def kappa_metric(t,x):
  16:         u = 0.5 * K.sum(K.square(x - t))
  17:         v = K.dot(K.transpose(x), t - K.mean(t))
  18:         return v / (v + u)
  19:     
  20:     def kappa_loss(t,x):
  21:         u = K.sum(K.square(x - t))
  22:         v = K.dot(K.squeeze(x,1), K.squeeze(t - K.mean(t),1))
  23:         return u / (2*v + u)
  24:     
  25:     lr = space['lr']
  26:     lr = lr*2
  27:     rho = space['rho']
  28:     clipnorm=space['clipnorm']
  29:     eps=1e-6
  30:     
  31:     opt = optimizers.RMSprop(lr=lr,
  32:                             rho=rho,
  33:                             clipnorm=clipnorm,
  34:                             epsilon=eps
  35:                             )
  36:     loss = kappa_loss
  37:     metric = kappa_metric
  38:     dataset.set_score_range(args.data_set)
  39:     evl = Evaluator(dataset, args.prompt_id, args.abs_out_path, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org, model_id=model_id)
  40:     
  41:     abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  42:     with open(abs_vocab_file, 'rb') as vocab_file:
  43:         vocab = pk.load(vocab_file)
  44:     
  45:     train_y_mean = train_y.mean(axis=0)
  46:     if train_y_mean.ndim == 0:
  47:         train_y_mean = np.expand_dims(train_y_mean, axis=1)
  48:     num_outputs = len(train_y_mean)
  49:     
  50:     mask_zero=False
  51:     
  52:     emb_dim = space['emb_dim']
  53:     rnn_dim = space['rnn_dim']
  54:     rnn_dim = int(rnn_dim)
  55:                    
  56:     model = Sequential()
  57:     model.add(Embedding(args.vocab_size, emb_dim, mask_zero=mask_zero))
  58:     model.add(RWA(rnn_dim))
  59:     model.add(Dense(num_outputs))
  60:     if not args.skip_init_bias:
  61:         bias_value = (np.log(train_y_mean) - np.log(1 - train_y_mean)).astype(K.floatx())
  62:         model.layers[-1].bias.set_value(bias_value)
  63:     model.add(Activation('tanh'))
  64:     model.emb_index = 0
  65:     
  66:     emb_path = 'embed/glove.6B.{}d.txt'.format(emb_dim)
  67:     abs_emb_path = args.abs_root + emb_path
  68:     
  69:     emb_reader = EmbReader(abs_emb_path, emb_dim=emb_dim)
  70:     emb_reader.load_embeddings(vocab)
  71:     emb_wts = emb_reader.get_emb_matrix_given_vocab(vocab, model.layers[model.emb_index].get_weights()[0])
  72:     wts = model.layers[model.emb_index].get_weights()
  73:     wts[0] = emb_wts
  74:     model.layers[model.emb_index].set_weights(wts)
  75: 
  76:     model.compile(loss=loss, optimizer=opt, metrics=[metric])
  77:     model_yaml = model.to_yaml()
  78:     print('model_id: %s' % (model_id))
  79:     print(model_yaml)
  80:     print('optimizer:    lr= %.4f, rho= %.4f, clipnorm= %.4f, epsilon= %.4f' % (lr, rho, clipnorm, eps))
  81:     
  82:     print('PARAMS\t\
  83:     %s\t\
  84:     lr= %.4f\t\
  85:     rho= %.4f\t\
  86:     clip= %.4f\t\
  87:     emb= %.4f\t\
  88:     rnn= %.4f' % (model_id, lr, rho, clipnorm, emb_dim, rnn_dim))
  89:     
  90:     for i in range(args.epochs):
  91:         train_history = model.fit(train_x, train_y, batch_size=args.batch_size, epochs=1, verbose=0)
  92:         evl.evaluate(model, i)
  93:         evl.output_info()
  94:         if i>5 and evl.dev_metric<0.4:
  95:             break
  96:         if i>10 and evl.dev_metric<0.5:
  97:             break
  98:         if i>15 and evl.dev_metric<0.6:
  99:             break
 100: 
 101:     best_dev_kappa = evl.best_dev
 102:     best_test_kappa = evl.best_test
 103: 
 104:     print('Test kappa:', best_dev_kappa)
 105:     return {'loss': 1-best_dev_kappa, 'status': STATUS_OK, 'model': model.to_yaml(), 'weights': pk.dumps(model.get_weights())}
 106: 
17/05/22 16:46:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:46:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KB, free 366.3 MB)
17/05/22 16:46:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.2.30:37218 (size: 3.8 KB, free: 366.3 MB)
17/05/22 16:46:06 INFO SparkContext: Created broadcast 0 from broadcast at PythonRDD.scala:482
17/05/22 16:46:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:46:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 114.0 B, free 366.3 MB)
17/05/22 16:46:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.2.30:37218 (size: 114.0 B, free: 366.3 MB)
17/05/22 16:46:06 INFO SparkContext: Created broadcast 1 from broadcast at PythonRDD.scala:482
17/05/22 16:46:07 INFO SparkContext: Starting job: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27
17/05/22 16:46:07 INFO DAGScheduler: Registering RDD 2 (coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:46:07 INFO DAGScheduler: Got job 0 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) with 2 output partitions
17/05/22 16:46:07 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:46:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/05/22 16:46:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/05/22 16:46:07 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents
17/05/22 16:46:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 366.3 MB)
17/05/22 16:46:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KB, free 366.3 MB)
17/05/22 16:46:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.2.30:37218 (size: 3.2 KB, free: 366.3 MB)
17/05/22 16:46:07 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/05/22 16:46:07 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:46:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/05/22 16:46:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:46:07 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:46:07 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 6462 bytes)
17/05/22 16:46:07 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:46:07 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:46:07 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:46:07 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:46:07 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 6508 bytes)
17/05/22 16:46:07 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/05/22 16:46:07 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/05/22 16:46:07 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/05/22 16:46:07 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/05/22 16:46:07 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/05/22 16:46:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/22 16:46:07 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/05/22 16:46:07 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/05/22 16:46:07 INFO Executor: Fetching file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495485966341
17/05/22 16:46:07 INFO Utils: /home/david/code/python/DeepATS/./elephas/run.py has been previously copied to /tmp/spark-0a1a159b-b2d2-45f0-ae22-98eb2384b53d/userFiles-786210f3-a364-4ff7-81f6-850b0648d8d7/run.py
17/05/22 16:46:07 INFO PythonRunner: Times: total = 199, boot = 192, init = 7, finish = 0
17/05/22 16:46:07 INFO PythonRunner: Times: total = 194, boot = 187, init = 7, finish = 0
17/05/22 16:46:07 INFO PythonRunner: Times: total = 216, boot = 206, init = 10, finish = 0
17/05/22 16:46:07 INFO PythonRunner: Times: total = 200, boot = 197, init = 2, finish = 1
17/05/22 16:46:07 INFO PythonRunner: Times: total = 216, boot = 200, init = 15, finish = 1
17/05/22 16:46:07 INFO PythonRunner: Times: total = 194, boot = 192, init = 2, finish = 0
17/05/22 16:46:07 INFO PythonRunner: Times: total = 190, boot = 185, init = 5, finish = 0
17/05/22 16:46:07 INFO PythonRunner: Times: total = 216, boot = 203, init = 13, finish = 0
17/05/22 16:46:07 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1969 bytes result sent to driver
17/05/22 16:46:07 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1969 bytes result sent to driver
17/05/22 16:46:07 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1969 bytes result sent to driver
17/05/22 16:46:07 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1969 bytes result sent to driver
17/05/22 16:46:07 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1969 bytes result sent to driver
17/05/22 16:46:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1969 bytes result sent to driver
17/05/22 16:46:07 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1969 bytes result sent to driver
17/05/22 16:46:07 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 445 ms on localhost (executor driver) (1/8)
17/05/22 16:46:07 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1969 bytes result sent to driver
17/05/22 16:46:07 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 444 ms on localhost (executor driver) (2/8)
17/05/22 16:46:07 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 445 ms on localhost (executor driver) (3/8)
17/05/22 16:46:07 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 451 ms on localhost (executor driver) (4/8)
17/05/22 16:46:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 476 ms on localhost (executor driver) (5/8)
17/05/22 16:46:07 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 448 ms on localhost (executor driver) (6/8)
17/05/22 16:46:07 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 455 ms on localhost (executor driver) (7/8)
17/05/22 16:46:07 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 450 ms on localhost (executor driver) (8/8)
17/05/22 16:46:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/05/22 16:46:07 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.490 s
17/05/22 16:46:07 INFO DAGScheduler: looking for newly runnable stages
17/05/22 16:46:07 INFO DAGScheduler: running: Set()
17/05/22 16:46:07 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/05/22 16:46:07 INFO DAGScheduler: failed: Set()
17/05/22 16:46:07 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27), which has no missing parents
17/05/22 16:46:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.1 KB, free 366.3 MB)
17/05/22 16:46:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KB, free 366.3 MB)
17/05/22 16:46:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.2.30:37218 (size: 7.7 KB, free: 366.3 MB)
17/05/22 16:46:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/05/22 16:46:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:46:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/05/22 16:46:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 6176 bytes)
17/05/22 16:46:07 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 6176 bytes)
17/05/22 16:46:07 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
17/05/22 16:46:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
17/05/22 16:46:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:46:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:46:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
17/05/22 16:46:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
Using Theano backend.
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
17/05/22 16:46:19 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 9)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/05/22 16:46:19 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 9, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

17/05/22 16:46:19 ERROR TaskSetManager: Task 1 in stage 1.0 failed 1 times; aborting job
17/05/22 16:46:19 INFO TaskSchedulerImpl: Cancelling stage 1
17/05/22 16:46:19 INFO Executor: Executor is trying to kill task 0.0 in stage 1.0 (TID 8)
17/05/22 16:46:19 INFO TaskSchedulerImpl: Stage 1 was cancelled
17/05/22 16:46:19 INFO DAGScheduler: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) failed in 11.861 s due to Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 9, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
17/05/22 16:46:19 INFO DAGScheduler: Job 0 failed: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27, took 12.431181 s
Traceback (most recent call last):
  File "/home/david/code/python/DeepATS/./elephas/run.py", line 190, in <module>
    best_model = hyperparam_model.minimize(model=model, data=data, max_evals=100)
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 32, in minimize
    trials_list = self.compute_trials(model, data, max_evals)
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 27, in compute_trials
    trials_list = dummy_rdd.mapPartitions(hyperas_worker.minimize).collect()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in collect
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 9, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:934)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

17/05/22 16:46:19 WARN PythonRunner: Incomplete task interrupted: Attempting to kill Python Worker
17/05/22 16:46:19 INFO Executor: Executor killed task 0.0 in stage 1.0 (TID 8)
17/05/22 16:46:19 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): TaskKilled (killed intentionally)
17/05/22 16:46:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/05/22 16:46:19 INFO SparkContext: Invoking stop() from shutdown hook
17/05/22 16:46:19 INFO SparkUI: Stopped Spark web UI at http://172.16.2.30:4040
17/05/22 16:46:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/05/22 16:46:19 INFO MemoryStore: MemoryStore cleared
17/05/22 16:46:19 INFO BlockManager: BlockManager stopped
17/05/22 16:46:19 INFO BlockManagerMaster: BlockManagerMaster stopped
17/05/22 16:46:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/05/22 16:46:19 INFO SparkContext: Successfully stopped SparkContext
17/05/22 16:46:19 INFO ShutdownHookManager: Shutdown hook called
17/05/22 16:46:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a1a159b-b2d2-45f0-ae22-98eb2384b53d
17/05/22 16:46:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a1a159b-b2d2-45f0-ae22-98eb2384b53d/pyspark-12486cc5-0bef-4f75-b768-76c04a0efbb9
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/05/22 16:54:31 WARN Utils: Your hostname, dvaughn-linux resolves to a loopback address: 127.0.1.1; using 172.16.2.30 instead (on interface eno1)
17/05/22 16:54:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.gof.compilelock): Overriding existing lock by dead process '5931' (I am process '6896')
Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
17/05/22 16:54:37 INFO SparkContext: Running Spark version 2.1.0
17/05/22 16:54:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/22 16:54:37 INFO SecurityManager: Changing view acls to: root
17/05/22 16:54:37 INFO SecurityManager: Changing modify acls to: root
17/05/22 16:54:37 INFO SecurityManager: Changing view acls groups to: 
17/05/22 16:54:37 INFO SecurityManager: Changing modify acls groups to: 
17/05/22 16:54:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/05/22 16:54:38 INFO Utils: Successfully started service 'sparkDriver' on port 33529.
17/05/22 16:54:38 INFO SparkEnv: Registering MapOutputTracker
17/05/22 16:54:38 INFO SparkEnv: Registering BlockManagerMaster
17/05/22 16:54:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/22 16:54:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/22 16:54:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e0ac1a47-fbe0-4d23-8b71-447ccf397a35
17/05/22 16:54:38 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/05/22 16:54:38 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/22 16:54:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/22 16:54:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.2.30:4040
17/05/22 16:54:38 INFO SparkContext: Added file file:/home/david/code/python/DeepATS/./elephas/run.py at file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495486478655
17/05/22 16:54:38 INFO Utils: Copying /home/david/code/python/DeepATS/elephas/run.py to /tmp/spark-fb410059-f786-4e54-954f-035baa6d6dad/userFiles-b0491bca-d9da-4ddb-b8a8-36c429cb4405/run.py
17/05/22 16:54:38 INFO Executor: Starting executor ID driver on host localhost
17/05/22 16:54:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33426.
17/05/22 16:54:38 INFO NettyBlockTransferService: Server created on 172.16.2.30:33426
17/05/22 16:54:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/22 16:54:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.30, 33426, None)
17/05/22 16:54:38 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.30:33426 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.30, 33426, None)
17/05/22 16:54:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.30, 33426, None)
17/05/22 16:54:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.30, 33426, None)
>>> Hyperas search space:

def get_space():
    return {
        'lr': hp.lognormal('lr', -3 * 2.3, .8),
        'rho': hp.normal('rho', .875, .04),
        'clipnorm': hp.uniform('clipnorm', 1, 15),
        'emb_dim': hp.choice('emb_dim', [50, 100, 200, 300]),
        'rnn_dim': hp.uniform('rnn_dim', 50, 300),
    }

>>> Data
   1: 
   2: from keras.utils import np_utils
   3: from keras.preprocessing import sequence
   4: import keras.backend as K
   5: import numpy as np
   6: 
   7: import pickle as pk
   8: import deepats.asap_reader as dataset
   9: from deepats.w2vEmbReader import W2VEmbReader as EmbReader
  10: from deepats.config import get_args
  11: 
  12: import logging
  13: logger = logging.getLogger(__name__)
  14: 
  15: args = get_args()
  16: 
  17: if args.seed > 0:
  18:     np.random.seed(args.seed)
  19:     
  20: emb_reader = EmbReader(args.emb_path, emb_dim=args.emb_dim)
  21: emb_words = emb_reader.load_words()
  22: 
  23: dataset.set_score_range(args.data_set)
  24: (train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data((args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path, min_word_freq=args.min_word_freq, emb_words=emb_words)
  25: 
  26: abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  27: with open(abs_vocab_file, 'wb') as vocab_file:
  28:     pk.dump(vocab, vocab_file)
  29: 
  30: train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)
  31: dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)
  32: test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)
  33: 
  34: train_y = np.array(train_y, dtype=K.floatx())
  35: dev_y = np.array(dev_y, dtype=K.floatx())
  36: test_y = np.array(test_y, dtype=K.floatx())
  37: 
  38: if args.prompt_id:
  39:     train_pmt = np.array(train_pmt, dtype='int32')
  40:     dev_pmt = np.array(dev_pmt, dtype='int32')
  41:     test_pmt = np.array(test_pmt, dtype='int32')
  42: 
  43: dev_y_org = dev_y.astype(dataset.get_ref_dtype())
  44: test_y_org = test_y.astype(dataset.get_ref_dtype())
  45: 
  46: train_y = dataset.get_model_friendly_scores(train_y, train_pmt)
  47: dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)
  48: test_y = dataset.get_model_friendly_scores(test_y, test_pmt)
  49: 
  50: 
  51: 
  52: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     
   4:     
   5:     def random_id(size=6, chars=string.ascii_uppercase + string.digits):
   6:         return ''.join(random.choice(chars) for _ in range(size))
   7:     
   8:     ms = int(round(time.time() * 1000))
   9:     rand_seed = ms % (2**32 - 1)
  10:     random.seed(rand_seed)
  11:        
  12:     args = get_args()
  13:     model_id = random_id()
  14:     
  15:     def kappa_metric(t,x):
  16:         u = 0.5 * K.sum(K.square(x - t))
  17:         v = K.dot(K.transpose(x), t - K.mean(t))
  18:         return v / (v + u)
  19:     
  20:     def kappa_loss(t,x):
  21:         u = K.sum(K.square(x - t))
  22:         v = K.dot(K.squeeze(x,1), K.squeeze(t - K.mean(t),1))
  23:         return u / (2*v + u)
  24:     
  25:     lr = space['lr']
  26:     lr = lr*2
  27:     rho = space['rho']
  28:     clipnorm=space['clipnorm']
  29:     eps=1e-6
  30:     
  31:     opt = optimizers.RMSprop(lr=lr,
  32:                             rho=rho,
  33:                             clipnorm=clipnorm,
  34:                             epsilon=eps
  35:                             )
  36:     loss = kappa_loss
  37:     metric = kappa_metric
  38:     dataset.set_score_range(args.data_set)
  39:     evl = Evaluator(dataset, args.prompt_id, args.abs_out_path, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org, model_id=model_id)
  40:     
  41:     abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  42:     with open(abs_vocab_file, 'rb') as vocab_file:
  43:         vocab = pk.load(vocab_file)
  44:     
  45:     train_y_mean = train_y.mean(axis=0)
  46:     if train_y_mean.ndim == 0:
  47:         train_y_mean = np.expand_dims(train_y_mean, axis=1)
  48:     num_outputs = len(train_y_mean)
  49:     
  50:     mask_zero=False
  51:     
  52:     emb_dim = space['emb_dim']
  53:     rnn_dim = space['rnn_dim']
  54:     rnn_dim = int(rnn_dim)
  55:                    
  56:     model = Sequential()
  57:     model.add(Embedding(args.vocab_size, emb_dim, mask_zero=mask_zero))
  58:     model.add(RWA(rnn_dim))
  59:     model.add(Dense(num_outputs))
  60:     if not args.skip_init_bias:
  61:         bias_value = (np.log(train_y_mean) - np.log(1 - train_y_mean)).astype(K.floatx())
  62:         model.layers[-1].bias.set_value(bias_value)
  63:     model.add(Activation('tanh'))
  64:     model.emb_index = 0
  65:     
  66:     emb_path = 'embed/glove.6B.{}d.txt'.format(emb_dim)
  67:     abs_emb_path = args.abs_root + emb_path
  68:     
  69:     emb_reader = EmbReader(abs_emb_path, emb_dim=emb_dim)
  70:     emb_reader.load_embeddings(vocab)
  71:     emb_wts = emb_reader.get_emb_matrix_given_vocab(vocab, model.layers[model.emb_index].get_weights()[0])
  72:     wts = model.layers[model.emb_index].get_weights()
  73:     wts[0] = emb_wts
  74:     model.layers[model.emb_index].set_weights(wts)
  75: 
  76:     model.compile(loss=loss, optimizer=opt, metrics=[metric])
  77:     model_yaml = model.to_yaml()
  78:     print('model_id: %s' % (model_id))
  79:     print(model_yaml)
  80:     print('optimizer:    lr= %.4f, rho= %.4f, clipnorm= %.4f, epsilon= %.4f' % (lr, rho, clipnorm, eps))
  81:     
  82:     print('PARAMS\t\
  83:     %s\t\
  84:     lr= %.4f\t\
  85:     rho= %.4f\t\
  86:     clip= %.4f\t\
  87:     emb= %.4f\t\
  88:     rnn= %.4f' % (model_id, lr, rho, clipnorm, emb_dim, rnn_dim))
  89:     
  90:     for i in range(args.epochs):
  91:         train_history = model.fit(train_x, train_y, batch_size=args.batch_size, epochs=1, verbose=0)
  92:         evl.evaluate(model, i)
  93:         evl.output_info()
  94:         if i>5 and evl.dev_metric<0.4:
  95:             break
  96:         if i>10 and evl.dev_metric<0.5:
  97:             break
  98:         if i>15 and evl.dev_metric<0.6:
  99:             break
 100: 
 101:     best_dev_kappa = evl.best_dev
 102:     best_test_kappa = evl.best_test
 103: 
 104:     print('Test kappa:', best_dev_kappa)
 105:     return {'loss': 1-best_dev_kappa, 'status': STATUS_OK, 'model': model.to_yaml(), 'weights': pk.dumps(model.get_weights())}
 106: 
17/05/22 16:54:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:54:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KB, free 366.3 MB)
17/05/22 16:54:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.2.30:33426 (size: 3.8 KB, free: 366.3 MB)
17/05/22 16:54:39 INFO SparkContext: Created broadcast 0 from broadcast at PythonRDD.scala:482
17/05/22 16:54:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:54:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 114.0 B, free 366.3 MB)
17/05/22 16:54:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.2.30:33426 (size: 114.0 B, free: 366.3 MB)
17/05/22 16:54:39 INFO SparkContext: Created broadcast 1 from broadcast at PythonRDD.scala:482
17/05/22 16:54:39 INFO SparkContext: Starting job: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27
17/05/22 16:54:39 INFO DAGScheduler: Registering RDD 2 (coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:54:39 INFO DAGScheduler: Got job 0 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) with 2 output partitions
17/05/22 16:54:39 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:54:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/05/22 16:54:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/05/22 16:54:39 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents
17/05/22 16:54:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 366.3 MB)
17/05/22 16:54:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KB, free 366.3 MB)
17/05/22 16:54:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.2.30:33426 (size: 3.2 KB, free: 366.3 MB)
17/05/22 16:54:39 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/05/22 16:54:39 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:54:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/05/22 16:54:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:54:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:54:39 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 6462 bytes)
17/05/22 16:54:39 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:54:39 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:54:39 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:54:39 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:54:39 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 6508 bytes)
17/05/22 16:54:39 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/05/22 16:54:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/22 16:54:39 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/05/22 16:54:39 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/05/22 16:54:39 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/05/22 16:54:39 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/05/22 16:54:39 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/05/22 16:54:39 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/05/22 16:54:39 INFO Executor: Fetching file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495486478655
17/05/22 16:54:39 INFO Utils: /home/david/code/python/DeepATS/./elephas/run.py has been previously copied to /tmp/spark-fb410059-f786-4e54-954f-035baa6d6dad/userFiles-b0491bca-d9da-4ddb-b8a8-36c429cb4405/run.py
17/05/22 16:54:39 INFO PythonRunner: Times: total = 182, boot = 179, init = 3, finish = 0
17/05/22 16:54:39 INFO PythonRunner: Times: total = 195, boot = 184, init = 10, finish = 1
17/05/22 16:54:39 INFO PythonRunner: Times: total = 183, boot = 178, init = 4, finish = 1
17/05/22 16:54:39 INFO PythonRunner: Times: total = 198, boot = 186, init = 12, finish = 0
17/05/22 16:54:39 INFO PythonRunner: Times: total = 183, boot = 176, init = 6, finish = 1
17/05/22 16:54:39 INFO PythonRunner: Times: total = 199, boot = 190, init = 9, finish = 0
17/05/22 16:54:39 INFO PythonRunner: Times: total = 196, boot = 182, init = 13, finish = 1
17/05/22 16:54:39 INFO PythonRunner: Times: total = 183, boot = 179, init = 3, finish = 1
17/05/22 16:54:39 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1969 bytes result sent to driver
17/05/22 16:54:39 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1969 bytes result sent to driver
17/05/22 16:54:39 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1969 bytes result sent to driver
17/05/22 16:54:39 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1969 bytes result sent to driver
17/05/22 16:54:39 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1969 bytes result sent to driver
17/05/22 16:54:39 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1969 bytes result sent to driver
17/05/22 16:54:39 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1969 bytes result sent to driver
17/05/22 16:54:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1969 bytes result sent to driver
17/05/22 16:54:39 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 440 ms on localhost (executor driver) (1/8)
17/05/22 16:54:39 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 447 ms on localhost (executor driver) (2/8)
17/05/22 16:54:39 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 449 ms on localhost (executor driver) (3/8)
17/05/22 16:54:39 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 444 ms on localhost (executor driver) (4/8)
17/05/22 16:54:39 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 453 ms on localhost (executor driver) (5/8)
17/05/22 16:54:39 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 449 ms on localhost (executor driver) (6/8)
17/05/22 16:54:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 479 ms on localhost (executor driver) (7/8)
17/05/22 16:54:39 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 445 ms on localhost (executor driver) (8/8)
17/05/22 16:54:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/05/22 16:54:39 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.490 s
17/05/22 16:54:39 INFO DAGScheduler: looking for newly runnable stages
17/05/22 16:54:39 INFO DAGScheduler: running: Set()
17/05/22 16:54:39 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/05/22 16:54:39 INFO DAGScheduler: failed: Set()
17/05/22 16:54:39 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27), which has no missing parents
17/05/22 16:54:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.1 KB, free 366.3 MB)
17/05/22 16:54:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KB, free 366.3 MB)
17/05/22 16:54:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.2.30:33426 (size: 7.7 KB, free: 366.3 MB)
17/05/22 16:54:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/05/22 16:54:39 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:54:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/05/22 16:54:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 6176 bytes)
17/05/22 16:54:39 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 6176 bytes)
17/05/22 16:54:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
17/05/22 16:54:39 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
17/05/22 16:54:39 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:54:39 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
17/05/22 16:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
Using Theano backend.
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
17/05/22 16:54:51 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 8)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/05/22 16:54:51 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

17/05/22 16:54:51 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
17/05/22 16:54:51 INFO TaskSchedulerImpl: Cancelling stage 1
17/05/22 16:54:51 INFO Executor: Executor is trying to kill task 1.0 in stage 1.0 (TID 9)
17/05/22 16:54:51 INFO TaskSchedulerImpl: Stage 1 was cancelled
17/05/22 16:54:51 INFO DAGScheduler: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) failed in 11.978 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
17/05/22 16:54:51 INFO DAGScheduler: Job 0 failed: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27, took 12.537938 s
Traceback (most recent call last):
  File "/home/david/code/python/DeepATS/./elephas/run.py", line 190, in <module>
    best_model = hyperparam_model.minimize(model=model, data=data, max_evals=100)
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 32, in minimize
    trials_list = self.compute_trials(model, data, max_evals)
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 27, in compute_trials
    trials_list = dummy_rdd.mapPartitions(hyperas_worker.minimize).collect()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in collect
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:934)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

17/05/22 16:54:51 WARN PythonRunner: Incomplete task interrupted: Attempting to kill Python Worker
17/05/22 16:54:51 INFO Executor: Executor killed task 1.0 in stage 1.0 (TID 9)
17/05/22 16:54:51 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 9, localhost, executor driver): TaskKilled (killed intentionally)
17/05/22 16:54:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/05/22 16:54:51 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.16.2.30:33426 in memory (size: 3.2 KB, free: 366.3 MB)
17/05/22 16:54:52 INFO SparkContext: Invoking stop() from shutdown hook
17/05/22 16:54:52 INFO SparkUI: Stopped Spark web UI at http://172.16.2.30:4040
17/05/22 16:54:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/05/22 16:54:52 INFO MemoryStore: MemoryStore cleared
17/05/22 16:54:52 INFO BlockManager: BlockManager stopped
17/05/22 16:54:52 INFO BlockManagerMaster: BlockManagerMaster stopped
17/05/22 16:54:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/05/22 16:54:52 INFO SparkContext: Successfully stopped SparkContext
17/05/22 16:54:52 INFO ShutdownHookManager: Shutdown hook called
17/05/22 16:54:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-fb410059-f786-4e54-954f-035baa6d6dad/pyspark-d49fa79e-e007-4610-964b-2edf1526237b
17/05/22 16:54:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-fb410059-f786-4e54-954f-035baa6d6dad
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/05/22 16:56:21 WARN Utils: Your hostname, dvaughn-linux resolves to a loopback address: 127.0.1.1; using 172.16.2.30 instead (on interface eno1)
17/05/22 16:56:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.gof.compilelock): Overriding existing lock by dead process '7108' (I am process '7419')
Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
17/05/22 16:56:27 INFO SparkContext: Running Spark version 2.1.0
17/05/22 16:56:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/22 16:56:27 INFO SecurityManager: Changing view acls to: root
17/05/22 16:56:27 INFO SecurityManager: Changing modify acls to: root
17/05/22 16:56:27 INFO SecurityManager: Changing view acls groups to: 
17/05/22 16:56:27 INFO SecurityManager: Changing modify acls groups to: 
17/05/22 16:56:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/05/22 16:56:27 INFO Utils: Successfully started service 'sparkDriver' on port 35811.
17/05/22 16:56:27 INFO SparkEnv: Registering MapOutputTracker
17/05/22 16:56:27 INFO SparkEnv: Registering BlockManagerMaster
17/05/22 16:56:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/22 16:56:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/22 16:56:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5c8f706c-33b1-4aa3-81c5-cf9367594251
17/05/22 16:56:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/05/22 16:56:27 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/22 16:56:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/22 16:56:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.2.30:4040
17/05/22 16:56:27 INFO SparkContext: Added file file:/home/david/code/python/DeepATS/./elephas/run.py at file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495486587946
17/05/22 16:56:27 INFO Utils: Copying /home/david/code/python/DeepATS/elephas/run.py to /tmp/spark-b0a0c66d-a66a-40c3-a944-43543cdc8344/userFiles-3cf2b173-4623-4594-a88a-1e915135837e/run.py
17/05/22 16:56:27 INFO Executor: Starting executor ID driver on host localhost
17/05/22 16:56:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43735.
17/05/22 16:56:28 INFO NettyBlockTransferService: Server created on 172.16.2.30:43735
17/05/22 16:56:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/22 16:56:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.30, 43735, None)
17/05/22 16:56:28 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.30:43735 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.30, 43735, None)
17/05/22 16:56:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.30, 43735, None)
17/05/22 16:56:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.30, 43735, None)
>>> Hyperas search space:

def get_space():
    return {
        'lr': hp.lognormal('lr', -3 * 2.3, .8),
        'rho': hp.normal('rho', .875, .04),
        'clipnorm': hp.uniform('clipnorm', 1, 15),
        'emb_dim': hp.choice('emb_dim', [50, 100, 200, 300]),
        'rnn_dim': hp.uniform('rnn_dim', 50, 300),
    }

>>> Data
   1: 
   2: from keras.utils import np_utils
   3: from keras.preprocessing import sequence
   4: import keras.backend as K
   5: import numpy as np
   6: 
   7: import pickle as pk
   8: import deepats.asap_reader as dataset
   9: from deepats.w2vEmbReader import W2VEmbReader as EmbReader
  10: from deepats.config import get_args
  11: 
  12: import logging
  13: logger = logging.getLogger(__name__)
  14: 
  15: args = get_args()
  16: 
  17: if args.seed > 0:
  18:     np.random.seed(args.seed)
  19:     
  20: emb_reader = EmbReader(args.emb_path, emb_dim=args.emb_dim)
  21: emb_words = emb_reader.load_words()
  22: 
  23: dataset.set_score_range(args.data_set)
  24: (train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data((args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path, min_word_freq=args.min_word_freq, emb_words=emb_words)
  25: 
  26: abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  27: with open(abs_vocab_file, 'wb') as vocab_file:
  28:     pk.dump(vocab, vocab_file)
  29: 
  30: train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)
  31: dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)
  32: test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)
  33: 
  34: train_y = np.array(train_y, dtype=K.floatx())
  35: dev_y = np.array(dev_y, dtype=K.floatx())
  36: test_y = np.array(test_y, dtype=K.floatx())
  37: 
  38: if args.prompt_id:
  39:     train_pmt = np.array(train_pmt, dtype='int32')
  40:     dev_pmt = np.array(dev_pmt, dtype='int32')
  41:     test_pmt = np.array(test_pmt, dtype='int32')
  42: 
  43: dev_y_org = dev_y.astype(dataset.get_ref_dtype())
  44: test_y_org = test_y.astype(dataset.get_ref_dtype())
  45: 
  46: train_y = dataset.get_model_friendly_scores(train_y, train_pmt)
  47: dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)
  48: test_y = dataset.get_model_friendly_scores(test_y, test_pmt)
  49: 
  50: 
  51: 
  52: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     
   4:     
   5:     def random_id(size=6, chars=string.ascii_uppercase + string.digits):
   6:         return ''.join(random.choice(chars) for _ in range(size))
   7:     
   8:     ms = int(round(time.time() * 1000))
   9:     rand_seed = ms % (2**32 - 1)
  10:     random.seed(rand_seed)
  11:        
  12:     args = get_args()
  13:     model_id = random_id()
  14:     
  15:     def kappa_metric(t,x):
  16:         u = 0.5 * K.sum(K.square(x - t))
  17:         v = K.dot(K.transpose(x), t - K.mean(t))
  18:         return v / (v + u)
  19:     
  20:     def kappa_loss(t,x):
  21:         u = K.sum(K.square(x - t))
  22:         v = K.dot(K.squeeze(x,1), K.squeeze(t - K.mean(t),1))
  23:         return u / (2*v + u)
  24:     
  25:     lr = space['lr']
  26:     lr = lr*2
  27:     rho = space['rho']
  28:     clipnorm=space['clipnorm']
  29:     eps=1e-6
  30:     
  31:     opt = optimizers.RMSprop(lr=lr,
  32:                             rho=rho,
  33:                             clipnorm=clipnorm,
  34:                             epsilon=eps
  35:                             )
  36:     loss = kappa_loss
  37:     metric = kappa_metric
  38:     dataset.set_score_range(args.data_set)
  39:     evl = Evaluator(dataset, args.prompt_id, args.abs_out_path, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org, model_id=model_id)
  40:     
  41:     abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  42:     with open(abs_vocab_file, 'rb') as vocab_file:
  43:         vocab = pk.load(vocab_file)
  44:     
  45:     train_y_mean = train_y.mean(axis=0)
  46:     if train_y_mean.ndim == 0:
  47:         train_y_mean = np.expand_dims(train_y_mean, axis=1)
  48:     num_outputs = len(train_y_mean)
  49:     
  50:     mask_zero=False
  51:     
  52:     emb_dim = space['emb_dim']
  53:     rnn_dim = space['rnn_dim']
  54:     rnn_dim = int(rnn_dim)
  55:                    
  56:     model = Sequential()
  57:     model.add(Embedding(args.vocab_size, emb_dim, mask_zero=mask_zero))
  58:     model.add(RWA(rnn_dim))
  59:     model.add(Dense(num_outputs))
  60:     if not args.skip_init_bias:
  61:         bias_value = (np.log(train_y_mean) - np.log(1 - train_y_mean)).astype(K.floatx())
  62:         model.layers[-1].bias.set_value(bias_value)
  63:     model.add(Activation('tanh'))
  64:     model.emb_index = 0
  65:     
  66:     emb_path = 'embed/glove.6B.{}d.txt'.format(emb_dim)
  67:     abs_emb_path = args.abs_root + emb_path
  68:     
  69:     emb_reader = EmbReader(abs_emb_path, emb_dim=emb_dim)
  70:     emb_reader.load_embeddings(vocab)
  71:     emb_wts = emb_reader.get_emb_matrix_given_vocab(vocab, model.layers[model.emb_index].get_weights()[0])
  72:     wts = model.layers[model.emb_index].get_weights()
  73:     wts[0] = emb_wts
  74:     model.layers[model.emb_index].set_weights(wts)
  75: 
  76:     model.compile(loss=loss, optimizer=opt, metrics=[metric])
  77:     model_yaml = model.to_yaml()
  78:     print('model_id: %s' % (model_id))
  79:     print(model_yaml)
  80:     print('optimizer:    lr= %.4f, rho= %.4f, clipnorm= %.4f, epsilon= %.4f' % (lr, rho, clipnorm, eps))
  81:     
  82:     print('PARAMS\t\
  83:     %s\t\
  84:     lr= %.4f\t\
  85:     rho= %.4f\t\
  86:     clip= %.4f\t\
  87:     emb= %.4f\t\
  88:     rnn= %.4f' % (model_id, lr, rho, clipnorm, emb_dim, rnn_dim))
  89:     
  90:     for i in range(args.epochs):
  91:         train_history = model.fit(train_x, train_y, batch_size=args.batch_size, epochs=1, verbose=0)
  92:         evl.evaluate(model, i)
  93:         evl.output_info()
  94:         if i>5 and evl.dev_metric<0.4:
  95:             break
  96:         if i>10 and evl.dev_metric<0.5:
  97:             break
  98:         if i>15 and evl.dev_metric<0.6:
  99:             break
 100: 
 101:     best_dev_kappa = evl.best_dev
 102:     best_test_kappa = evl.best_test
 103: 
 104:     print('Test kappa:', best_dev_kappa)
 105:     return {'loss': 1-best_dev_kappa, 'status': STATUS_OK, 'model': model.to_yaml(), 'weights': pk.dumps(model.get_weights())}
 106: 
17/05/22 16:56:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:56:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KB, free 366.3 MB)
17/05/22 16:56:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.2.30:43735 (size: 3.8 KB, free: 366.3 MB)
17/05/22 16:56:28 INFO SparkContext: Created broadcast 0 from broadcast at PythonRDD.scala:482
17/05/22 16:56:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:56:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 114.0 B, free 366.3 MB)
17/05/22 16:56:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.2.30:43735 (size: 114.0 B, free: 366.3 MB)
17/05/22 16:56:28 INFO SparkContext: Created broadcast 1 from broadcast at PythonRDD.scala:482
17/05/22 16:56:28 INFO SparkContext: Starting job: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27
17/05/22 16:56:28 INFO DAGScheduler: Registering RDD 2 (coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:56:28 INFO DAGScheduler: Got job 0 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) with 2 output partitions
17/05/22 16:56:28 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:56:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/05/22 16:56:28 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/05/22 16:56:28 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents
17/05/22 16:56:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 366.3 MB)
17/05/22 16:56:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KB, free 366.3 MB)
17/05/22 16:56:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.2.30:43735 (size: 3.2 KB, free: 366.3 MB)
17/05/22 16:56:28 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/05/22 16:56:28 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:56:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/05/22 16:56:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:56:28 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:56:28 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 6462 bytes)
17/05/22 16:56:28 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:56:28 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:56:28 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:56:28 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:56:28 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 6508 bytes)
17/05/22 16:56:28 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/05/22 16:56:28 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/05/22 16:56:28 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/05/22 16:56:28 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/05/22 16:56:28 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/05/22 16:56:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/22 16:56:28 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/05/22 16:56:28 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/05/22 16:56:28 INFO Executor: Fetching file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495486587946
17/05/22 16:56:28 INFO Utils: /home/david/code/python/DeepATS/./elephas/run.py has been previously copied to /tmp/spark-b0a0c66d-a66a-40c3-a944-43543cdc8344/userFiles-3cf2b173-4623-4594-a88a-1e915135837e/run.py
17/05/22 16:56:29 INFO PythonRunner: Times: total = 201, boot = 199, init = 2, finish = 0
17/05/22 16:56:29 INFO PythonRunner: Times: total = 206, boot = 203, init = 3, finish = 0
17/05/22 16:56:29 INFO PythonRunner: Times: total = 220, boot = 218, init = 2, finish = 0
17/05/22 16:56:29 INFO PythonRunner: Times: total = 189, boot = 188, init = 1, finish = 0
17/05/22 16:56:29 INFO PythonRunner: Times: total = 192, boot = 186, init = 6, finish = 0
17/05/22 16:56:29 INFO PythonRunner: Times: total = 224, boot = 223, init = 1, finish = 0
17/05/22 16:56:29 INFO PythonRunner: Times: total = 192, boot = 190, init = 2, finish = 0
17/05/22 16:56:29 INFO PythonRunner: Times: total = 196, boot = 194, init = 2, finish = 0
17/05/22 16:56:29 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1969 bytes result sent to driver
17/05/22 16:56:29 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1969 bytes result sent to driver
17/05/22 16:56:29 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1969 bytes result sent to driver
17/05/22 16:56:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1969 bytes result sent to driver
17/05/22 16:56:29 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1969 bytes result sent to driver
17/05/22 16:56:29 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1969 bytes result sent to driver
17/05/22 16:56:29 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1969 bytes result sent to driver
17/05/22 16:56:29 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1969 bytes result sent to driver
17/05/22 16:56:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 473 ms on localhost (executor driver) (1/8)
17/05/22 16:56:29 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 448 ms on localhost (executor driver) (2/8)
17/05/22 16:56:29 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 442 ms on localhost (executor driver) (3/8)
17/05/22 16:56:29 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 440 ms on localhost (executor driver) (4/8)
17/05/22 16:56:29 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 452 ms on localhost (executor driver) (5/8)
17/05/22 16:56:29 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 439 ms on localhost (executor driver) (6/8)
17/05/22 16:56:29 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 442 ms on localhost (executor driver) (7/8)
17/05/22 16:56:29 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 447 ms on localhost (executor driver) (8/8)
17/05/22 16:56:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/05/22 16:56:29 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.491 s
17/05/22 16:56:29 INFO DAGScheduler: looking for newly runnable stages
17/05/22 16:56:29 INFO DAGScheduler: running: Set()
17/05/22 16:56:29 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/05/22 16:56:29 INFO DAGScheduler: failed: Set()
17/05/22 16:56:29 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27), which has no missing parents
17/05/22 16:56:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.1 KB, free 366.3 MB)
17/05/22 16:56:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KB, free 366.3 MB)
17/05/22 16:56:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.2.30:43735 (size: 7.7 KB, free: 366.3 MB)
17/05/22 16:56:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/05/22 16:56:29 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:56:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/05/22 16:56:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 6176 bytes)
17/05/22 16:56:29 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 6176 bytes)
17/05/22 16:56:29 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
17/05/22 16:56:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
17/05/22 16:56:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:56:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:56:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
17/05/22 16:56:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
Using Theano backend.
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
model_id: R4IYJN
backend: !!python/unicode 'theano'
class_name: Sequential
config:
- class_name: Embedding
  config:
    activity_regularizer: null
    batch_input_shape: !!python/tuple [null, null]
    dtype: int32
    embeddings_constraint: null
    embeddings_initializer:
      class_name: RandomUniform
      config: {maxval: 0.05, minval: -0.05, seed: null}
    embeddings_regularizer: null
    input_dim: 4000
    input_length: null
    mask_zero: false
    name: embedding_1
    output_dim: 100
    trainable: true
- class_name: RWA
  config:
    activation: tanh
    average_constraint: null
    average_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    average_regularizer: null
    bias_constraint: null
    bias_initializer:
      class_name: Zeros
      config: {}
    bias_regularizer: null
    features_constraint: null
    features_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    features_regularizer: null
    go_backwards: false
    implementation: 0
    initial_attention_constraint: null
    initial_attention_initializer:
      class_name: Zeros
      config: {}
    initial_attention_regularizer: null
    name: rwa_1
    recurrent_activation: relu
    recurrent_constraint: null
    recurrent_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    recurrent_regularizer: null
    return_sequences: false
    stateful: false
    trainable: true
    units: 79
    unroll: false
- class_name: Dense
  config:
    activation: linear
    activity_regularizer: null
    bias_constraint: null
    bias_initializer:
      class_name: Zeros
      config: {}
    bias_regularizer: null
    kernel_constraint: null
    kernel_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    kernel_regularizer: null
    name: dense_1
    trainable: true
    units: 1
    use_bias: true
- class_name: Activation
  config: {activation: tanh, name: activation_1, trainable: true}
keras_version: 2.0.4

optimizer:    lr= 0.0021, rho= 0.8626, clipnorm= 1.0581, epsilon= 0.0000
PARAMS	    R4IYJN	    lr= 0.0021	    rho= 0.8626	    clip= 1.0581	    emb= 100.0000	    rnn= 79.0000
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/05/22 16:58:36 WARN Utils: Your hostname, dvaughn-linux resolves to a loopback address: 127.0.1.1; using 172.16.2.30 instead (on interface eno1)
17/05/22 16:58:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
17/05/22 16:58:41 INFO SparkContext: Running Spark version 2.1.0
17/05/22 16:58:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/22 16:58:42 INFO SecurityManager: Changing view acls to: root
17/05/22 16:58:42 INFO SecurityManager: Changing modify acls to: root
17/05/22 16:58:42 INFO SecurityManager: Changing view acls groups to: 
17/05/22 16:58:42 INFO SecurityManager: Changing modify acls groups to: 
17/05/22 16:58:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/05/22 16:58:42 INFO Utils: Successfully started service 'sparkDriver' on port 32775.
17/05/22 16:58:42 INFO SparkEnv: Registering MapOutputTracker
17/05/22 16:58:42 INFO SparkEnv: Registering BlockManagerMaster
17/05/22 16:58:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/22 16:58:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/22 16:58:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2789dc89-bf61-4cce-afb1-2b5a5e774baa
17/05/22 16:58:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/05/22 16:58:42 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/22 16:58:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/22 16:58:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.2.30:4040
17/05/22 16:58:42 INFO SparkContext: Added file file:/home/david/code/python/DeepATS/./elephas/run.py at file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495486722912
17/05/22 16:58:42 INFO Utils: Copying /home/david/code/python/DeepATS/elephas/run.py to /tmp/spark-d5bb2255-5869-436e-8dd6-804dd1e6cedf/userFiles-26aa27f8-b9de-4862-a0f9-19c034b8d9e2/run.py
17/05/22 16:58:42 INFO Executor: Starting executor ID driver on host localhost
17/05/22 16:58:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45563.
17/05/22 16:58:42 INFO NettyBlockTransferService: Server created on 172.16.2.30:45563
17/05/22 16:58:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/22 16:58:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.30, 45563, None)
17/05/22 16:58:42 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.30:45563 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.30, 45563, None)
17/05/22 16:58:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.30, 45563, None)
17/05/22 16:58:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.30, 45563, None)
>>> Hyperas search space:

def get_space():
    return {
        'lr': hp.lognormal('lr', -3 * 2.3, .8),
        'rho': hp.normal('rho', .875, .04),
        'clipnorm': hp.uniform('clipnorm', 1, 15),
        'emb_dim': hp.choice('emb_dim', [50, 100, 200, 300]),
        'rnn_dim': hp.uniform('rnn_dim', 50, 300),
    }

>>> Data
   1: 
   2: from keras.utils import np_utils
   3: from keras.preprocessing import sequence
   4: import keras.backend as K
   5: import numpy as np
   6: 
   7: import pickle as pk
   8: import deepats.asap_reader as dataset
   9: from deepats.w2vEmbReader import W2VEmbReader as EmbReader
  10: from deepats.config import get_args
  11: 
  12: import logging
  13: logger = logging.getLogger(__name__)
  14: 
  15: args = get_args()
  16: 
  17: if args.seed > 0:
  18:     np.random.seed(args.seed)
  19:     
  20: emb_reader = EmbReader(args.emb_path, emb_dim=args.emb_dim)
  21: emb_words = emb_reader.load_words()
  22: 
  23: dataset.set_score_range(args.data_set)
  24: (train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data((args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path, min_word_freq=args.min_word_freq, emb_words=emb_words)
  25: 
  26: abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  27: with open(abs_vocab_file, 'wb') as vocab_file:
  28:     pk.dump(vocab, vocab_file)
  29: 
  30: train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)
  31: dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)
  32: test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)
  33: 
  34: train_y = np.array(train_y, dtype=K.floatx())
  35: dev_y = np.array(dev_y, dtype=K.floatx())
  36: test_y = np.array(test_y, dtype=K.floatx())
  37: 
  38: if args.prompt_id:
  39:     train_pmt = np.array(train_pmt, dtype='int32')
  40:     dev_pmt = np.array(dev_pmt, dtype='int32')
  41:     test_pmt = np.array(test_pmt, dtype='int32')
  42: 
  43: dev_y_org = dev_y.astype(dataset.get_ref_dtype())
  44: test_y_org = test_y.astype(dataset.get_ref_dtype())
  45: 
  46: train_y = dataset.get_model_friendly_scores(train_y, train_pmt)
  47: dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)
  48: test_y = dataset.get_model_friendly_scores(test_y, test_pmt)
  49: 
  50: 
  51: 
  52: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     
   4:     
   5:     def random_id(size=6, chars=string.ascii_uppercase + string.digits):
   6:         return ''.join(random.choice(chars) for _ in range(size))
   7:     
   8:     ms = int(round(time.time() * 1000))
   9:     rand_seed = ms % (2**32 - 1)
  10:     random.seed(rand_seed)
  11:        
  12:     args = get_args()
  13:     model_id = random_id()
  14:     
  15:     def kappa_metric(t,x):
  16:         u = 0.5 * K.sum(K.square(x - t))
  17:         v = K.dot(K.transpose(x), t - K.mean(t))
  18:         return v / (v + u)
  19:     
  20:     def kappa_loss(t,x):
  21:         u = K.sum(K.square(x - t))
  22:         v = K.dot(K.squeeze(x,1), K.squeeze(t - K.mean(t),1))
  23:         return u / (2*v + u)
  24:     
  25:     lr = space['lr']
  26:     lr = lr*2
  27:     rho = space['rho']
  28:     clipnorm=space['clipnorm']
  29:     eps=1e-6
  30:     
  31:     opt = optimizers.RMSprop(lr=lr,
  32:                             rho=rho,
  33:                             clipnorm=clipnorm,
  34:                             epsilon=eps
  35:                             )
  36:     loss = kappa_loss
  37:     metric = kappa_metric
  38:     dataset.set_score_range(args.data_set)
  39:     evl = Evaluator(dataset, args.prompt_id, args.abs_out_path, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org, model_id=model_id)
  40:     
  41:     abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  42:     with open(abs_vocab_file, 'rb') as vocab_file:
  43:         vocab = pk.load(vocab_file)
  44:     
  45:     train_y_mean = train_y.mean(axis=0)
  46:     if train_y_mean.ndim == 0:
  47:         train_y_mean = np.expand_dims(train_y_mean, axis=1)
  48:     num_outputs = len(train_y_mean)
  49:     
  50:     mask_zero=False
  51:     
  52:     emb_dim = space['emb_dim']
  53:     rnn_dim = space['rnn_dim']
  54:     rnn_dim = int(rnn_dim)
  55:                    
  56:     model = Sequential()
  57:     model.add(Embedding(args.vocab_size, emb_dim, mask_zero=mask_zero))
  58:     model.add(RWA(rnn_dim))
  59:     model.add(Dense(num_outputs))
  60:     if not args.skip_init_bias:
  61:         bias_value = (np.log(train_y_mean) - np.log(1 - train_y_mean)).astype(K.floatx())
  62:         model.layers[-1].bias.set_value(bias_value)
  63:     model.add(Activation('tanh'))
  64:     model.emb_index = 0
  65:     
  66:     emb_path = 'embed/glove.6B.{}d.txt'.format(emb_dim)
  67:     abs_emb_path = args.abs_root + emb_path
  68:     
  69:     emb_reader = EmbReader(abs_emb_path, emb_dim=emb_dim)
  70:     emb_reader.load_embeddings(vocab)
  71:     emb_wts = emb_reader.get_emb_matrix_given_vocab(vocab, model.layers[model.emb_index].get_weights()[0])
  72:     wts = model.layers[model.emb_index].get_weights()
  73:     wts[0] = emb_wts
  74:     model.layers[model.emb_index].set_weights(wts)
  75: 
  76:     model.compile(loss=loss, optimizer=opt, metrics=[metric])
  77:     model_yaml = model.to_yaml()
  78:     print('model_id: %s' % (model_id))
  79:     print(model_yaml)
  80:     print('optimizer:    lr= %.4f, rho= %.4f, clipnorm= %.4f, epsilon= %.4f' % (lr, rho, clipnorm, eps))
  81:     
  82:     print('PARAMS\t\
  83:     %s\t\
  84:     lr= %.4f\t\
  85:     rho= %.4f\t\
  86:     clip= %.4f\t\
  87:     emb= %.4f\t\
  88:     rnn= %.4f' % (model_id, lr, rho, clipnorm, emb_dim, rnn_dim))
  89:     
  90:     for i in range(args.epochs):
  91:         train_history = model.fit(train_x, train_y, batch_size=args.batch_size, epochs=1, verbose=0)
  92:         evl.evaluate(model, i)
  93:         evl.output_info()
  94:         if i>5 and evl.dev_metric<0.4:
  95:             break
  96:         if i>10 and evl.dev_metric<0.5:
  97:             break
  98:         if i>15 and evl.dev_metric<0.6:
  99:             break
 100: 
 101:     best_dev_kappa = evl.best_dev
 102:     best_test_kappa = evl.best_test
 103: 
 104:     print('Test kappa:', best_dev_kappa)
 105:     return {'loss': 1-best_dev_kappa, 'status': STATUS_OK, 'model': model.to_yaml(), 'weights': pk.dumps(model.get_weights())}
 106: 
17/05/22 16:58:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:58:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KB, free 366.3 MB)
17/05/22 16:58:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.2.30:45563 (size: 3.8 KB, free: 366.3 MB)
17/05/22 16:58:43 INFO SparkContext: Created broadcast 0 from broadcast at PythonRDD.scala:482
17/05/22 16:58:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:58:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 114.0 B, free 366.3 MB)
17/05/22 16:58:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.2.30:45563 (size: 114.0 B, free: 366.3 MB)
17/05/22 16:58:43 INFO SparkContext: Created broadcast 1 from broadcast at PythonRDD.scala:482
17/05/22 16:58:43 INFO SparkContext: Starting job: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27
17/05/22 16:58:43 INFO DAGScheduler: Registering RDD 2 (coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:58:43 INFO DAGScheduler: Got job 0 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) with 2 output partitions
17/05/22 16:58:43 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:58:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/05/22 16:58:43 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/05/22 16:58:43 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents
17/05/22 16:58:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 366.3 MB)
17/05/22 16:58:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KB, free 366.3 MB)
17/05/22 16:58:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.2.30:45563 (size: 3.2 KB, free: 366.3 MB)
17/05/22 16:58:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/05/22 16:58:43 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:58:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/05/22 16:58:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:58:43 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:58:43 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 6462 bytes)
17/05/22 16:58:43 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:58:43 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:58:43 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:58:43 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:58:43 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 6508 bytes)
17/05/22 16:58:43 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/05/22 16:58:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/22 16:58:43 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/05/22 16:58:43 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/05/22 16:58:43 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/05/22 16:58:43 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/05/22 16:58:43 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/05/22 16:58:43 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/05/22 16:58:43 INFO Executor: Fetching file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495486722912
17/05/22 16:58:43 INFO Utils: /home/david/code/python/DeepATS/./elephas/run.py has been previously copied to /tmp/spark-d5bb2255-5869-436e-8dd6-804dd1e6cedf/userFiles-26aa27f8-b9de-4862-a0f9-19c034b8d9e2/run.py
17/05/22 16:58:44 INFO PythonRunner: Times: total = 208, boot = 193, init = 15, finish = 0
17/05/22 16:58:44 INFO PythonRunner: Times: total = 214, boot = 203, init = 11, finish = 0
17/05/22 16:58:44 INFO PythonRunner: Times: total = 214, boot = 200, init = 14, finish = 0
17/05/22 16:58:44 INFO PythonRunner: Times: total = 197, boot = 191, init = 5, finish = 1
17/05/22 16:58:44 INFO PythonRunner: Times: total = 195, boot = 193, init = 2, finish = 0
17/05/22 16:58:44 INFO PythonRunner: Times: total = 196, boot = 188, init = 7, finish = 1
17/05/22 16:58:44 INFO PythonRunner: Times: total = 213, boot = 196, init = 17, finish = 0
17/05/22 16:58:44 INFO PythonRunner: Times: total = 196, boot = 187, init = 9, finish = 0
17/05/22 16:58:44 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1969 bytes result sent to driver
17/05/22 16:58:44 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1969 bytes result sent to driver
17/05/22 16:58:44 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1969 bytes result sent to driver
17/05/22 16:58:44 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1969 bytes result sent to driver
17/05/22 16:58:44 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1969 bytes result sent to driver
17/05/22 16:58:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1969 bytes result sent to driver
17/05/22 16:58:44 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1969 bytes result sent to driver
17/05/22 16:58:44 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1969 bytes result sent to driver
17/05/22 16:58:44 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 443 ms on localhost (executor driver) (1/8)
17/05/22 16:58:44 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 445 ms on localhost (executor driver) (2/8)
17/05/22 16:58:44 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 442 ms on localhost (executor driver) (3/8)
17/05/22 16:58:44 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 446 ms on localhost (executor driver) (4/8)
17/05/22 16:58:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 474 ms on localhost (executor driver) (5/8)
17/05/22 16:58:44 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 453 ms on localhost (executor driver) (6/8)
17/05/22 16:58:44 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 452 ms on localhost (executor driver) (7/8)
17/05/22 16:58:44 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 446 ms on localhost (executor driver) (8/8)
17/05/22 16:58:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/05/22 16:58:44 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.488 s
17/05/22 16:58:44 INFO DAGScheduler: looking for newly runnable stages
17/05/22 16:58:44 INFO DAGScheduler: running: Set()
17/05/22 16:58:44 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/05/22 16:58:44 INFO DAGScheduler: failed: Set()
17/05/22 16:58:44 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27), which has no missing parents
17/05/22 16:58:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.1 KB, free 366.3 MB)
17/05/22 16:58:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KB, free 366.3 MB)
17/05/22 16:58:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.2.30:45563 (size: 7.7 KB, free: 366.3 MB)
17/05/22 16:58:44 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/05/22 16:58:44 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:58:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/05/22 16:58:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 6176 bytes)
17/05/22 16:58:44 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 6176 bytes)
17/05/22 16:58:44 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
17/05/22 16:58:44 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
17/05/22 16:58:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:58:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:58:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
17/05/22 16:58:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
17/05/22 16:58:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.16.2.30:45563 in memory (size: 3.2 KB, free: 366.3 MB)
Using Theano backend.
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
17/05/22 16:58:55 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 8)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/05/22 16:58:55 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

17/05/22 16:58:55 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
17/05/22 16:58:55 INFO TaskSchedulerImpl: Cancelling stage 1
17/05/22 16:58:55 INFO Executor: Executor is trying to kill task 1.0 in stage 1.0 (TID 9)
17/05/22 16:58:55 INFO TaskSchedulerImpl: Stage 1 was cancelled
17/05/22 16:58:55 INFO DAGScheduler: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) failed in 11.658 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
17/05/22 16:58:55 INFO DAGScheduler: Job 0 failed: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27, took 12.222821 s
Traceback (most recent call last):
  File "/home/david/code/python/DeepATS/./elephas/run.py", line 190, in <module>
    best_model = hyperparam_model.minimize(model=model, data=data, max_evals=100)
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 32, in minimize
    trials_list = self.compute_trials(model, data, max_evals)
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 27, in compute_trials
    trials_list = dummy_rdd.mapPartitions(hyperas_worker.minimize).collect()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in collect
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 8, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:934)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/david/.linuxbrew/Cellar/apache-spark/2.1.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/david/code/python/elephas/elephas/hyperparam.py", line 90, in minimize
    rseed=rand_seed
  File "/home/david/code/python/hyperas/hyperas/optim.py", line 176, in base_minimizer
    rstate=np.random.RandomState(rseed))
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 307, in fmin
    return_argmin=return_argmin,
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 635, in fmin
    return_argmin=return_argmin)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 320, in fmin
    rval.exhaust()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 199, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.async)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 173, in run
    self.serial_evaluate()
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.py", line 92, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/usr/local/lib/python2.7/dist-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.py", line 840, in evaluate
    rval = self.fn(pyll_rval)
  File "temp_model.py", line 253, in keras_fmin_fnct
  File "deepats/asap_evaluator.py", line 31, in __init__
    self.dump_ref_scores()
  File "deepats/asap_evaluator.py", line 40, in dump_ref_scores
    np.savetxt(self.out_dir + '/preds/dev_ref.txt', self.dev_y_org, fmt='%i')
  File "/home/david/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 1155, in savetxt
    fh = open(fname, 'w')
IOError: [Errno 2] No such file or directory: '/home/david/code/python/DeepATS/output/preds/dev_ref.txt'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

17/05/22 16:58:56 INFO SparkContext: Invoking stop() from shutdown hook
17/05/22 16:58:56 INFO SparkUI: Stopped Spark web UI at http://172.16.2.30:4040
17/05/22 16:58:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/05/22 16:58:56 WARN PythonRunner: Incomplete task interrupted: Attempting to kill Python Worker
17/05/22 16:58:56 INFO Executor: Executor killed task 1.0 in stage 1.0 (TID 9)
17/05/22 16:58:56 INFO MemoryStore: MemoryStore cleared
17/05/22 16:58:56 INFO BlockManager: BlockManager stopped
17/05/22 16:58:56 INFO BlockManagerMaster: BlockManagerMaster stopped
17/05/22 16:58:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/05/22 16:58:56 INFO SparkContext: Successfully stopped SparkContext
17/05/22 16:58:56 INFO ShutdownHookManager: Shutdown hook called
17/05/22 16:58:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-d5bb2255-5869-436e-8dd6-804dd1e6cedf
17/05/22 16:58:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-d5bb2255-5869-436e-8dd6-804dd1e6cedf/pyspark-a484c2ec-7aa8-47f6-85ab-aabb8e0e195a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/05/22 16:59:07 WARN Utils: Your hostname, dvaughn-linux resolves to a loopback address: 127.0.1.1; using 172.16.2.30 instead (on interface eno1)
17/05/22 16:59:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.gof.compilelock): Overriding existing lock by dead process '8122' (I am process '8359')
Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
17/05/22 16:59:12 INFO SparkContext: Running Spark version 2.1.0
17/05/22 16:59:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/22 16:59:13 INFO SecurityManager: Changing view acls to: root
17/05/22 16:59:13 INFO SecurityManager: Changing modify acls to: root
17/05/22 16:59:13 INFO SecurityManager: Changing view acls groups to: 
17/05/22 16:59:13 INFO SecurityManager: Changing modify acls groups to: 
17/05/22 16:59:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/05/22 16:59:13 INFO Utils: Successfully started service 'sparkDriver' on port 32877.
17/05/22 16:59:13 INFO SparkEnv: Registering MapOutputTracker
17/05/22 16:59:13 INFO SparkEnv: Registering BlockManagerMaster
17/05/22 16:59:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/22 16:59:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/22 16:59:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-329d86bb-6b15-42da-aeb9-6a441a21ea8a
17/05/22 16:59:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/05/22 16:59:13 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/22 16:59:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/22 16:59:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.2.30:4040
17/05/22 16:59:13 INFO SparkContext: Added file file:/home/david/code/python/DeepATS/./elephas/run.py at file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495486753624
17/05/22 16:59:13 INFO Utils: Copying /home/david/code/python/DeepATS/elephas/run.py to /tmp/spark-d7d15a02-0252-46d1-9a47-c9b96638db46/userFiles-555e5c10-ef20-4659-b03f-7fd964ad16f2/run.py
17/05/22 16:59:13 INFO Executor: Starting executor ID driver on host localhost
17/05/22 16:59:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43719.
17/05/22 16:59:13 INFO NettyBlockTransferService: Server created on 172.16.2.30:43719
17/05/22 16:59:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/22 16:59:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.30, 43719, None)
17/05/22 16:59:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.30:43719 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.30, 43719, None)
17/05/22 16:59:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.30, 43719, None)
17/05/22 16:59:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.30, 43719, None)
>>> Hyperas search space:

def get_space():
    return {
        'lr': hp.lognormal('lr', -3 * 2.3, .8),
        'rho': hp.normal('rho', .875, .04),
        'clipnorm': hp.uniform('clipnorm', 1, 15),
        'emb_dim': hp.choice('emb_dim', [50, 100, 200, 300]),
        'rnn_dim': hp.uniform('rnn_dim', 50, 300),
    }

>>> Data
   1: 
   2: from keras.utils import np_utils
   3: from keras.preprocessing import sequence
   4: import keras.backend as K
   5: import numpy as np
   6: 
   7: import pickle as pk
   8: import deepats.asap_reader as dataset
   9: from deepats.w2vEmbReader import W2VEmbReader as EmbReader
  10: from deepats.config import get_args
  11: 
  12: import logging
  13: logger = logging.getLogger(__name__)
  14: 
  15: args = get_args()
  16: 
  17: if args.seed > 0:
  18:     np.random.seed(args.seed)
  19:     
  20: emb_reader = EmbReader(args.emb_path, emb_dim=args.emb_dim)
  21: emb_words = emb_reader.load_words()
  22: 
  23: dataset.set_score_range(args.data_set)
  24: (train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data((args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path, min_word_freq=args.min_word_freq, emb_words=emb_words)
  25: 
  26: abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  27: with open(abs_vocab_file, 'wb') as vocab_file:
  28:     pk.dump(vocab, vocab_file)
  29: 
  30: train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)
  31: dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)
  32: test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)
  33: 
  34: train_y = np.array(train_y, dtype=K.floatx())
  35: dev_y = np.array(dev_y, dtype=K.floatx())
  36: test_y = np.array(test_y, dtype=K.floatx())
  37: 
  38: if args.prompt_id:
  39:     train_pmt = np.array(train_pmt, dtype='int32')
  40:     dev_pmt = np.array(dev_pmt, dtype='int32')
  41:     test_pmt = np.array(test_pmt, dtype='int32')
  42: 
  43: dev_y_org = dev_y.astype(dataset.get_ref_dtype())
  44: test_y_org = test_y.astype(dataset.get_ref_dtype())
  45: 
  46: train_y = dataset.get_model_friendly_scores(train_y, train_pmt)
  47: dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)
  48: test_y = dataset.get_model_friendly_scores(test_y, test_pmt)
  49: 
  50: 
  51: 
  52: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     
   4:     
   5:     def random_id(size=6, chars=string.ascii_uppercase + string.digits):
   6:         return ''.join(random.choice(chars) for _ in range(size))
   7:     
   8:     ms = int(round(time.time() * 1000))
   9:     rand_seed = ms % (2**32 - 1)
  10:     random.seed(rand_seed)
  11:        
  12:     args = get_args()
  13:     model_id = random_id()
  14:     
  15:     def kappa_metric(t,x):
  16:         u = 0.5 * K.sum(K.square(x - t))
  17:         v = K.dot(K.transpose(x), t - K.mean(t))
  18:         return v / (v + u)
  19:     
  20:     def kappa_loss(t,x):
  21:         u = K.sum(K.square(x - t))
  22:         v = K.dot(K.squeeze(x,1), K.squeeze(t - K.mean(t),1))
  23:         return u / (2*v + u)
  24:     
  25:     lr = space['lr']
  26:     lr = lr*2
  27:     rho = space['rho']
  28:     clipnorm=space['clipnorm']
  29:     eps=1e-6
  30:     
  31:     opt = optimizers.RMSprop(lr=lr,
  32:                             rho=rho,
  33:                             clipnorm=clipnorm,
  34:                             epsilon=eps
  35:                             )
  36:     loss = kappa_loss
  37:     metric = kappa_metric
  38:     dataset.set_score_range(args.data_set)
  39:     evl = Evaluator(dataset, args.prompt_id, args.abs_out_path, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org, model_id=model_id)
  40:     
  41:     abs_vocab_file = args.abs_out_path + '/vocab.pkl'
  42:     with open(abs_vocab_file, 'rb') as vocab_file:
  43:         vocab = pk.load(vocab_file)
  44:     
  45:     train_y_mean = train_y.mean(axis=0)
  46:     if train_y_mean.ndim == 0:
  47:         train_y_mean = np.expand_dims(train_y_mean, axis=1)
  48:     num_outputs = len(train_y_mean)
  49:     
  50:     mask_zero=False
  51:     
  52:     emb_dim = space['emb_dim']
  53:     rnn_dim = space['rnn_dim']
  54:     rnn_dim = int(rnn_dim)
  55:                    
  56:     model = Sequential()
  57:     model.add(Embedding(args.vocab_size, emb_dim, mask_zero=mask_zero))
  58:     model.add(RWA(rnn_dim))
  59:     model.add(Dense(num_outputs))
  60:     if not args.skip_init_bias:
  61:         bias_value = (np.log(train_y_mean) - np.log(1 - train_y_mean)).astype(K.floatx())
  62:         model.layers[-1].bias.set_value(bias_value)
  63:     model.add(Activation('tanh'))
  64:     model.emb_index = 0
  65:     
  66:     emb_path = 'embed/glove.6B.{}d.txt'.format(emb_dim)
  67:     abs_emb_path = args.abs_root + emb_path
  68:     
  69:     emb_reader = EmbReader(abs_emb_path, emb_dim=emb_dim)
  70:     emb_reader.load_embeddings(vocab)
  71:     emb_wts = emb_reader.get_emb_matrix_given_vocab(vocab, model.layers[model.emb_index].get_weights()[0])
  72:     wts = model.layers[model.emb_index].get_weights()
  73:     wts[0] = emb_wts
  74:     model.layers[model.emb_index].set_weights(wts)
  75: 
  76:     model.compile(loss=loss, optimizer=opt, metrics=[metric])
  77:     model_yaml = model.to_yaml()
  78:     print('model_id: %s' % (model_id))
  79:     print(model_yaml)
  80:     print('optimizer:    lr= %.4f, rho= %.4f, clipnorm= %.4f, epsilon= %.4f' % (lr, rho, clipnorm, eps))
  81:     
  82:     print('PARAMS\t\
  83:     %s\t\
  84:     lr= %.4f\t\
  85:     rho= %.4f\t\
  86:     clip= %.4f\t\
  87:     emb= %.4f\t\
  88:     rnn= %.4f' % (model_id, lr, rho, clipnorm, emb_dim, rnn_dim))
  89:     
  90:     for i in range(args.epochs):
  91:         train_history = model.fit(train_x, train_y, batch_size=args.batch_size, epochs=1, verbose=0)
  92:         evl.evaluate(model, i)
  93:         evl.output_info()
  94:         if i>5 and evl.dev_metric<0.4:
  95:             break
  96:         if i>10 and evl.dev_metric<0.5:
  97:             break
  98:         if i>15 and evl.dev_metric<0.6:
  99:             break
 100: 
 101:     best_dev_kappa = evl.best_dev
 102:     best_test_kappa = evl.best_test
 103: 
 104:     print('Test kappa:', best_dev_kappa)
 105:     return {'loss': 1-best_dev_kappa, 'status': STATUS_OK, 'model': model.to_yaml(), 'weights': pk.dumps(model.get_weights())}
 106: 
17/05/22 16:59:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:59:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KB, free 366.3 MB)
17/05/22 16:59:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.2.30:43719 (size: 3.8 KB, free: 366.3 MB)
17/05/22 16:59:13 INFO SparkContext: Created broadcast 0 from broadcast at PythonRDD.scala:482
17/05/22 16:59:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 296.0 B, free 366.3 MB)
17/05/22 16:59:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 114.0 B, free 366.3 MB)
17/05/22 16:59:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.2.30:43719 (size: 114.0 B, free: 366.3 MB)
17/05/22 16:59:14 INFO SparkContext: Created broadcast 1 from broadcast at PythonRDD.scala:482
17/05/22 16:59:14 INFO SparkContext: Starting job: collect at /home/david/code/python/elephas/elephas/hyperparam.py:27
17/05/22 16:59:14 INFO DAGScheduler: Registering RDD 2 (coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:59:14 INFO DAGScheduler: Got job 0 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27) with 2 output partitions
17/05/22 16:59:14 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:59:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/05/22 16:59:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/05/22 16:59:14 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents
17/05/22 16:59:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 366.3 MB)
17/05/22 16:59:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KB, free 366.3 MB)
17/05/22 16:59:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.2.30:43719 (size: 3.2 KB, free: 366.3 MB)
17/05/22 16:59:14 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/05/22 16:59:14 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at coalesce at NativeMethodAccessorImpl.java:0)
17/05/22 16:59:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/05/22 16:59:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:59:14 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6345 bytes)
17/05/22 16:59:14 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 6462 bytes)
17/05/22 16:59:14 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:59:14 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:59:14 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:59:14 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 6469 bytes)
17/05/22 16:59:14 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 6508 bytes)
17/05/22 16:59:14 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/05/22 16:59:14 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/05/22 16:59:14 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/05/22 16:59:14 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/05/22 16:59:14 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/05/22 16:59:14 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/05/22 16:59:14 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/05/22 16:59:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/22 16:59:14 INFO Executor: Fetching file:/home/david/code/python/DeepATS/./elephas/run.py with timestamp 1495486753624
17/05/22 16:59:14 INFO Utils: /home/david/code/python/DeepATS/./elephas/run.py has been previously copied to /tmp/spark-d7d15a02-0252-46d1-9a47-c9b96638db46/userFiles-555e5c10-ef20-4659-b03f-7fd964ad16f2/run.py
17/05/22 16:59:14 INFO PythonRunner: Times: total = 213, boot = 204, init = 8, finish = 1
17/05/22 16:59:14 INFO PythonRunner: Times: total = 213, boot = 197, init = 15, finish = 1
17/05/22 16:59:14 INFO PythonRunner: Times: total = 193, boot = 184, init = 9, finish = 0
17/05/22 16:59:14 INFO PythonRunner: Times: total = 193, boot = 188, init = 5, finish = 0
17/05/22 16:59:14 INFO PythonRunner: Times: total = 213, boot = 207, init = 5, finish = 1
17/05/22 16:59:14 INFO PythonRunner: Times: total = 198, boot = 184, init = 14, finish = 0
17/05/22 16:59:14 INFO PythonRunner: Times: total = 200, boot = 189, init = 11, finish = 0
17/05/22 16:59:14 INFO PythonRunner: Times: total = 193, boot = 186, init = 7, finish = 0
17/05/22 16:59:14 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1969 bytes result sent to driver
17/05/22 16:59:14 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1969 bytes result sent to driver
17/05/22 16:59:14 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1969 bytes result sent to driver
17/05/22 16:59:14 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1969 bytes result sent to driver
17/05/22 16:59:14 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1969 bytes result sent to driver
17/05/22 16:59:14 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1969 bytes result sent to driver
17/05/22 16:59:14 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1969 bytes result sent to driver
17/05/22 16:59:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1969 bytes result sent to driver
17/05/22 16:59:14 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 442 ms on localhost (executor driver) (1/8)
17/05/22 16:59:14 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 438 ms on localhost (executor driver) (2/8)
17/05/22 16:59:14 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 435 ms on localhost (executor driver) (3/8)
17/05/22 16:59:14 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 442 ms on localhost (executor driver) (4/8)
17/05/22 16:59:14 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 437 ms on localhost (executor driver) (5/8)
17/05/22 16:59:14 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 432 ms on localhost (executor driver) (6/8)
17/05/22 16:59:14 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 434 ms on localhost (executor driver) (7/8)
17/05/22 16:59:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 477 ms on localhost (executor driver) (8/8)
17/05/22 16:59:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/05/22 16:59:14 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.496 s
17/05/22 16:59:14 INFO DAGScheduler: looking for newly runnable stages
17/05/22 16:59:14 INFO DAGScheduler: running: Set()
17/05/22 16:59:14 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/05/22 16:59:14 INFO DAGScheduler: failed: Set()
17/05/22 16:59:14 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27), which has no missing parents
17/05/22 16:59:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.1 KB, free 366.3 MB)
17/05/22 16:59:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KB, free 366.3 MB)
17/05/22 16:59:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.2.30:43719 (size: 7.7 KB, free: 366.3 MB)
17/05/22 16:59:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/05/22 16:59:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/david/code/python/elephas/elephas/hyperparam.py:27)
17/05/22 16:59:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/05/22 16:59:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 6176 bytes)
17/05/22 16:59:14 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 6176 bytes)
17/05/22 16:59:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
17/05/22 16:59:14 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
17/05/22 16:59:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:59:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
17/05/22 16:59:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
17/05/22 16:59:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
Using Theano backend.
Using Theano backend.
WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)
No handlers could be found for logger "deepats.asap_reader"
model_id: V9N9JS
backend: !!python/unicode 'theano'
class_name: Sequential
config:
- class_name: Embedding
  config:
    activity_regularizer: null
    batch_input_shape: !!python/tuple [null, null]
    dtype: int32
    embeddings_constraint: null
    embeddings_initializer:
      class_name: RandomUniform
      config: {maxval: 0.05, minval: -0.05, seed: null}
    embeddings_regularizer: null
    input_dim: 4000
    input_length: null
    mask_zero: false
    name: embedding_1
    output_dim: 200
    trainable: true
- class_name: RWA
  config:
    activation: tanh
    average_constraint: null
    average_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    average_regularizer: null
    bias_constraint: null
    bias_initializer:
      class_name: Zeros
      config: {}
    bias_regularizer: null
    features_constraint: null
    features_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    features_regularizer: null
    go_backwards: false
    implementation: 0
    initial_attention_constraint: null
    initial_attention_initializer:
      class_name: Zeros
      config: {}
    initial_attention_regularizer: null
    name: rwa_1
    recurrent_activation: relu
    recurrent_constraint: null
    recurrent_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    recurrent_regularizer: null
    return_sequences: false
    stateful: false
    trainable: true
    units: 101
    unroll: false
- class_name: Dense
  config:
    activation: linear
    activity_regularizer: null
    bias_constraint: null
    bias_initializer:
      class_name: Zeros
      config: {}
    bias_regularizer: null
    kernel_constraint: null
    kernel_initializer:
      class_name: VarianceScaling
      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}
    kernel_regularizer: null
    name: dense_1
    trainable: true
    units: 1
    use_bias: true
- class_name: Activation
  config: {activation: tanh, name: activation_1, trainable: true}
keras_version: 2.0.4

optimizer:    lr= 0.0010, rho= 0.9105, clipnorm= 7.1031, epsilon= 0.0000
PARAMS	    V9N9JS	    lr= 0.0010	    rho= 0.9105	    clip= 7.1031	    emb= 200.0000	    rnn= 101.0000
